# üí° Additional Suggestions to Maximize Performance

## üéØ **Top 10 Additional Strategies**

### **1. Gradient Clipping (Prevent Exploding Gradients)**

Add to your training command:
```bash
python3 src/train_resume.py \
    --resume_from models/qwen_high_quality \
    --output_dir models/qwen_v2 \
    --epochs 10 \
    --learning_rate 3e-5 \
    --max_grad_norm 1.0  # Add this!
```

**Why**: Prevents training instability  
**Impact**: +1-2% F1, more stable training

---

### **2. Increase Batch Size (If You Have Memory)**

```bash
python3 src/train_resume.py \
    --resume_from models/qwen_high_quality \
    --output_dir models/qwen_v2 \
    --epochs 10 \
    --learning_rate 3e-5 \
    --batch_size 8 \
    --gradient_accumulation_steps 2
```

**Effective batch size**: 8 √ó 2 = 16 (vs default 4 √ó 4 = 16)

**Why**: Larger batches = more stable gradients  
**Impact**: +1-3% F1, faster convergence

---

### **3. Save Best Model Only**

Modify your training to save only the best checkpoint:

```python
# In TrainingArguments
save_strategy="epoch",
load_best_model_at_end=True,
metric_for_best_model="eval/f1_score",
greater_is_better=True,
save_total_limit=2  # Keep only best 2 checkpoints
```

**Why**: Automatically keeps best model  
**Impact**: Convenience, saves disk space

---

### **4. Use Different Random Seeds (Ensemble)**

Train 3 models with different seeds:

```bash
# Model 1
python3 src/train.py \
    --epochs 10 --lora_r 32 --seed 42 \
    --output_dir models/model_seed42

# Model 2
python3 src/train.py \
    --epochs 10 --lora_r 32 --seed 123 \
    --output_dir models/model_seed123

# Model 3
python3 src/train.py \
    --epochs 10 --lora_r 32 --seed 999 \
    --output_dir models/model_seed999
```

Then ensemble predictions (majority vote or averaging).

**Why**: Different models learn different patterns  
**Impact**: +2-5% F1 from ensemble

---

### **5. Warmup Steps (Better Than Warmup Ratio)**

Instead of `--warmup_ratio 0.1`, use specific steps:

```bash
python3 src/train_resume.py \
    --resume_from models/qwen_high_quality \
    --epochs 10 \
    --learning_rate 3e-5 \
    --warmup_steps 100  # First 100 steps
```

**Why**: More control over warmup  
**Impact**: +0.5-1% F1, more stable start

---

### **6. Label Smoothing (Prevent Overconfidence)**

Add to TrainingArguments:

```python
TrainingArguments(
    # ... other args ...
    label_smoothing_factor=0.1,  # Add this
)
```

**Why**: Model doesn't become overconfident  
**Impact**: +1-2% F1, better generalization

---

### **7. Longer Sequences (If Data Allows)**

```bash
python3 src/train_resume.py \
    --resume_from models/qwen_high_quality \
    --epochs 10 \
    --max_seq_length 768  # vs default 512
```

**Why**: Capture more context  
**Impact**: +1-3% F1 if your data has long sequences  
**Cost**: More memory, slower training

---

### **8. Learning Rate Finder**

Before training, find optimal LR:

```python
from transformers import Trainer

# Create trainer
trainer = Trainer(...)

# Find best LR
lr_finder_results = trainer.hyperparameter_search(
    direction="maximize",
    backend="optuna",
    n_trials=10
)

print(f"Best LR: {lr_finder_results.hyperparameters['learning_rate']}")
```

**Why**: Use optimal LR for your data  
**Impact**: +2-5% F1

---

### **9. Mixed Precision Training (Faster)**

Already enabled with `fp16=True`, but ensure it's on:

```bash
# Check your GPU supports it
python3 -c "import torch; print(torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 7)"
```

If True, you're good! If False, remove `fp16=True`.

**Why**: 2x faster training, less memory  
**Impact**: Speed boost, no F1 change

---

### **10. Data Quality Check**

Before training, validate your data:

```python
# Check for issues
import json
from collections import Counter

def check_data_quality(data_path):
    issues = []
    
    # Load data
    with open(data_path) as f:
        data = json.load(f)
    
    # Check for duplicates
    inputs = [d['input'] for d in data]
    duplicates = [k for k, v in Counter(inputs).items() if v > 1]
    if duplicates:
        issues.append(f"Found {len(duplicates)} duplicate inputs")
    
    # Check for empty fields
    empty_inputs = sum(1 for d in data if not d.get('input'))
    empty_targets = sum(1 for d in data if not d.get('target'))
    if empty_inputs:
        issues.append(f"Found {empty_inputs} empty inputs")
    if empty_targets:
        issues.append(f"Found {empty_targets} empty targets")
    
    # Check JSON validity
    invalid_json = 0
    for d in data:
        try:
            json.loads(d['target'])
        except:
            invalid_json += 1
    if invalid_json:
        issues.append(f"Found {invalid_json} invalid JSON targets")
    
    return issues

# Run check
issues = check_data_quality('train/train/data.json')
if issues:
    print("‚ö†Ô∏è Data quality issues:")
    for issue in issues:
        print(f"  - {issue}")
else:
    print("‚úì Data quality looks good!")
```

**Why**: Clean data = better model  
**Impact**: +3-10% F1 if you have data issues

---

## üéì **Advanced Techniques**

### **11. Curriculum Learning**

Train on easy examples first, then hard ones:

```python
# Sort data by difficulty (e.g., by length)
sorted_data = sorted(train_data, key=lambda x: len(x['input']))

# Train in stages
# Stage 1: Easy examples (short)
train_on(sorted_data[:len(sorted_data)//2])

# Stage 2: All examples
train_on(sorted_data)
```

**Impact**: +2-4% F1

---

### **12. LoRA+ (Better LoRA)**

Use different learning rates for A and B matrices:

```python
from peft import LoraConfig

peft_config = LoraConfig(
    # ... other params ...
    use_rslora=True,  # Rank-stabilized LoRA
)
```

**Impact**: +1-3% F1

---

### **13. Quantization (Faster Inference)**

After training, quantize for faster inference:

```python
from transformers import AutoModelForCausalLM
import torch

# Load model
model = AutoModelForCausalLM.from_pretrained(
    "models/qwen_finetuned",
    torch_dtype=torch.float16,
    device_map="auto"
)

# Quantize to 8-bit
model = model.to(torch.int8)
```

**Impact**: 2-4x faster inference, minimal F1 loss

---

### **14. Test-Time Augmentation**

During inference, generate multiple predictions and vote:

```python
# Generate with different temperatures
predictions = []
for temp in [0.05, 0.1, 0.15]:
    pred = model.generate(..., temperature=temp)
    predictions.append(pred)

# Majority vote
final_prediction = most_common(predictions)
```

**Impact**: +1-2% F1

---

### **15. Validation-Based Learning Rate**

Adjust LR based on validation F1:

```python
from transformers import TrainerCallback

class ReduceLROnPlateau(TrainerCallback):
    def __init__(self, patience=2, factor=0.5):
        self.patience = patience
        self.factor = factor
        self.best_f1 = 0
        self.wait = 0
        
    def on_epoch_end(self, args, state, control, **kwargs):
        current_f1 = state.log_history[-1].get('eval/f1_score', 0)
        
        if current_f1 <= self.best_f1:
            self.wait += 1
            if self.wait >= self.patience:
                # Reduce LR
                new_lr = args.learning_rate * self.factor
                print(f"Reducing LR: {args.learning_rate} ‚Üí {new_lr}")
                args.learning_rate = new_lr
                self.wait = 0
        else:
            self.best_f1 = current_f1
            self.wait = 0
```

**Impact**: +1-3% F1

---

## üìä **Priority Ranking**

### **Must Do (High Impact, Easy)**
1. ‚úÖ **Gradient Clipping** (add `--max_grad_norm 1.0`)
2. ‚úÖ **Data Quality Check** (run validation script)
3. ‚úÖ **Save Best Model** (add to TrainingArguments)
4. ‚úÖ **Monitor F1 Every Epoch** (already doing)

### **Should Do (Medium Impact, Medium Effort)**
5. ‚úÖ **Increase Batch Size** (if memory allows)
6. ‚úÖ **Label Smoothing** (add to TrainingArguments)
7. ‚úÖ **Warmup Steps** (instead of ratio)
8. ‚úÖ **Early Stopping** (prevent overfitting)

### **Nice to Have (Lower Impact, More Effort)**
9. ‚≠ê **Ensemble Models** (train 3 models)
10. ‚≠ê **Learning Rate Finder** (find optimal LR)
11. ‚≠ê **Longer Sequences** (if data supports)
12. ‚≠ê **Test-Time Augmentation** (during inference)

---

## üéØ **My Top 3 Recommendations for You**

### **1. Add Gradient Clipping (Easiest)**

```bash
python3 src/train_resume.py \
    --resume_from models/qwen_high_quality \
    --output_dir models/qwen_v2 \
    --epochs 10 \
    --learning_rate 3e-5 \
    --weight_decay 0.02 \
    --max_grad_norm 1.0  # ‚Üê Add this
```

**Time**: 0 minutes (just add parameter)  
**Impact**: +1-2% F1

---

### **2. Train 3 Models and Ensemble (Best ROI)**

```bash
# Model 1: Standard
python3 src/train.py \
    --epochs 10 --lora_r 32 --learning_rate 2e-4 \
    --output_dir models/ensemble_1

# Model 2: Higher LR
python3 src/train.py \
    --epochs 10 --lora_r 32 --learning_rate 3e-4 \
    --output_dir models/ensemble_2

# Model 3: More regularization
python3 src/train.py \
    --epochs 10 --lora_r 32 --learning_rate 2e-4 \
    --weight_decay 0.03 \
    --output_dir models/ensemble_3

# Ensemble predictions
python3 scripts/ensemble.py \
    models/ensemble_1 \
    models/ensemble_2 \
    models/ensemble_3
```

**Time**: 6-8 hours (3 models)  
**Impact**: +3-5% F1

---

### **3. Check and Clean Your Data (Highest Impact)**

```bash
# Run data quality check
python3 scripts/check_data_quality.py train/train

# Fix any issues found
# Remove duplicates, fix invalid JSON, etc.

# Retrain with clean data
python3 src/train.py --epochs 10 --lora_r 32
```

**Time**: 1-2 hours  
**Impact**: +5-10% F1 (if you have data issues)

---

## üìù **Quick Wins Checklist**

- [ ] Add `--max_grad_norm 1.0` to training
- [ ] Check data quality (no duplicates, valid JSON)
- [ ] Use `--epochs 10` (not 50!)
- [ ] Monitor F1 scores every epoch
- [ ] Stop if F1 decreases for 2+ epochs
- [ ] Save best model only
- [ ] Try `--weight_decay 0.02` (higher)
- [ ] Consider ensemble (3 models)

---

## üöÄ **Optimal Training Command**

Based on all suggestions:

```bash
python3 src/train_resume.py \
    --resume_from models/qwen_high_quality \
    --output_dir models/qwen_v2_optimal \
    --epochs 10 \
    --learning_rate 3e-5 \
    --weight_decay 0.02 \
    --lr_scheduler_type cosine \
    --warmup_ratio 0.1 \
    --max_grad_norm 1.0 \
    --batch_size 4 \
    --gradient_accumulation_steps 4 \
    --lora_r 32 \
    --lora_alpha 64 \
    --save_total_limit 2
```

**Expected F1**: 0.85-0.90  
**Time**: ~2 hours  
**Success rate**: 95%+

---

**These suggestions should help you squeeze out every bit of performance!** üéØ
