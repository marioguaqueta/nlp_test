{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 10.0,
  "eval_steps": 500,
  "global_step": 5070,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0019753086419753087,
      "grad_norm": 0.6651736497879028,
      "learning_rate": 0.0,
      "loss": 2.5971,
      "step": 1
    },
    {
      "epoch": 0.019753086419753086,
      "grad_norm": 0.6910882592201233,
      "learning_rate": 2.3653088042049936e-06,
      "loss": 2.6166,
      "step": 10
    },
    {
      "epoch": 0.03950617283950617,
      "grad_norm": 0.6625962853431702,
      "learning_rate": 4.993429697766097e-06,
      "loss": 2.6031,
      "step": 20
    },
    {
      "epoch": 0.05925925925925926,
      "grad_norm": 0.6262690424919128,
      "learning_rate": 7.621550591327202e-06,
      "loss": 2.556,
      "step": 30
    },
    {
      "epoch": 0.07901234567901234,
      "grad_norm": 0.5850425958633423,
      "learning_rate": 1.0249671484888305e-05,
      "loss": 2.5338,
      "step": 40
    },
    {
      "epoch": 0.09876543209876543,
      "grad_norm": 0.5538790225982666,
      "learning_rate": 1.287779237844941e-05,
      "loss": 2.5016,
      "step": 50
    },
    {
      "epoch": 0.11851851851851852,
      "grad_norm": 0.546364963054657,
      "learning_rate": 1.5505913272010515e-05,
      "loss": 2.4559,
      "step": 60
    },
    {
      "epoch": 0.1382716049382716,
      "grad_norm": 0.5304038524627686,
      "learning_rate": 1.8134034165571616e-05,
      "loss": 2.4192,
      "step": 70
    },
    {
      "epoch": 0.1580246913580247,
      "grad_norm": 0.5252860188484192,
      "learning_rate": 2.076215505913272e-05,
      "loss": 2.3537,
      "step": 80
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 0.5213155150413513,
      "learning_rate": 2.3390275952693823e-05,
      "loss": 2.2933,
      "step": 90
    },
    {
      "epoch": 0.19753086419753085,
      "grad_norm": 0.5143474340438843,
      "learning_rate": 2.6018396846254928e-05,
      "loss": 2.2436,
      "step": 100
    },
    {
      "epoch": 0.21728395061728395,
      "grad_norm": 0.5803279876708984,
      "learning_rate": 2.8646517739816033e-05,
      "loss": 2.1704,
      "step": 110
    },
    {
      "epoch": 0.23703703703703705,
      "grad_norm": 0.5886192321777344,
      "learning_rate": 3.127463863337714e-05,
      "loss": 2.1437,
      "step": 120
    },
    {
      "epoch": 0.25679012345679014,
      "grad_norm": 0.621412456035614,
      "learning_rate": 3.390275952693824e-05,
      "loss": 2.0789,
      "step": 130
    },
    {
      "epoch": 0.2765432098765432,
      "grad_norm": 0.6781103014945984,
      "learning_rate": 3.653088042049935e-05,
      "loss": 2.0128,
      "step": 140
    },
    {
      "epoch": 0.2962962962962963,
      "grad_norm": 0.7246180772781372,
      "learning_rate": 3.915900131406045e-05,
      "loss": 2.0036,
      "step": 150
    },
    {
      "epoch": 0.3160493827160494,
      "grad_norm": 0.7556523680686951,
      "learning_rate": 4.178712220762156e-05,
      "loss": 1.932,
      "step": 160
    },
    {
      "epoch": 0.3358024691358025,
      "grad_norm": 0.7441836595535278,
      "learning_rate": 4.441524310118266e-05,
      "loss": 1.8995,
      "step": 170
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 0.7886531352996826,
      "learning_rate": 4.704336399474376e-05,
      "loss": 1.9191,
      "step": 180
    },
    {
      "epoch": 0.37530864197530867,
      "grad_norm": 0.8205556273460388,
      "learning_rate": 4.967148488830486e-05,
      "loss": 1.8856,
      "step": 190
    },
    {
      "epoch": 0.3950617283950617,
      "grad_norm": 0.8815603852272034,
      "learning_rate": 5.229960578186597e-05,
      "loss": 1.8698,
      "step": 200
    },
    {
      "epoch": 0.4148148148148148,
      "grad_norm": 0.8925701975822449,
      "learning_rate": 5.492772667542707e-05,
      "loss": 1.8223,
      "step": 210
    },
    {
      "epoch": 0.4345679012345679,
      "grad_norm": 0.9281020760536194,
      "learning_rate": 5.755584756898817e-05,
      "loss": 1.8126,
      "step": 220
    },
    {
      "epoch": 0.454320987654321,
      "grad_norm": 0.9828532934188843,
      "learning_rate": 6.018396846254928e-05,
      "loss": 1.716,
      "step": 230
    },
    {
      "epoch": 0.4740740740740741,
      "grad_norm": 0.9319187998771667,
      "learning_rate": 6.281208935611038e-05,
      "loss": 1.7883,
      "step": 240
    },
    {
      "epoch": 0.49382716049382713,
      "grad_norm": 1.0296568870544434,
      "learning_rate": 6.544021024967148e-05,
      "loss": 1.7575,
      "step": 250
    },
    {
      "epoch": 0.5135802469135803,
      "grad_norm": 1.0207973718643188,
      "learning_rate": 6.806833114323259e-05,
      "loss": 1.7555,
      "step": 260
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 1.0178090333938599,
      "learning_rate": 7.069645203679369e-05,
      "loss": 1.7204,
      "step": 270
    },
    {
      "epoch": 0.5530864197530864,
      "grad_norm": 1.0545713901519775,
      "learning_rate": 7.33245729303548e-05,
      "loss": 1.7279,
      "step": 280
    },
    {
      "epoch": 0.5728395061728395,
      "grad_norm": 1.052119255065918,
      "learning_rate": 7.59526938239159e-05,
      "loss": 1.7432,
      "step": 290
    },
    {
      "epoch": 0.5925925925925926,
      "grad_norm": 0.9691841006278992,
      "learning_rate": 7.8580814717477e-05,
      "loss": 1.7283,
      "step": 300
    },
    {
      "epoch": 0.6123456790123457,
      "grad_norm": 1.0712275505065918,
      "learning_rate": 8.12089356110381e-05,
      "loss": 1.6998,
      "step": 310
    },
    {
      "epoch": 0.6320987654320988,
      "grad_norm": 1.0354022979736328,
      "learning_rate": 8.383705650459922e-05,
      "loss": 1.7113,
      "step": 320
    },
    {
      "epoch": 0.6518518518518519,
      "grad_norm": 1.002575159072876,
      "learning_rate": 8.646517739816032e-05,
      "loss": 1.6925,
      "step": 330
    },
    {
      "epoch": 0.671604938271605,
      "grad_norm": 1.043230414390564,
      "learning_rate": 8.909329829172142e-05,
      "loss": 1.6727,
      "step": 340
    },
    {
      "epoch": 0.691358024691358,
      "grad_norm": 1.0535316467285156,
      "learning_rate": 9.172141918528253e-05,
      "loss": 1.6397,
      "step": 350
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 1.1212553977966309,
      "learning_rate": 9.434954007884364e-05,
      "loss": 1.6486,
      "step": 360
    },
    {
      "epoch": 0.7308641975308642,
      "grad_norm": 1.024672269821167,
      "learning_rate": 9.697766097240474e-05,
      "loss": 1.6441,
      "step": 370
    },
    {
      "epoch": 0.7506172839506173,
      "grad_norm": 1.1351089477539062,
      "learning_rate": 9.960578186596584e-05,
      "loss": 1.6103,
      "step": 380
    },
    {
      "epoch": 0.7703703703703704,
      "grad_norm": 1.0616838932037354,
      "learning_rate": 0.00010223390275952693,
      "loss": 1.6468,
      "step": 390
    },
    {
      "epoch": 0.7901234567901234,
      "grad_norm": 1.1019238233566284,
      "learning_rate": 0.00010486202365308803,
      "loss": 1.6599,
      "step": 400
    },
    {
      "epoch": 0.8098765432098766,
      "grad_norm": 1.0247570276260376,
      "learning_rate": 0.00010749014454664913,
      "loss": 1.6117,
      "step": 410
    },
    {
      "epoch": 0.8296296296296296,
      "grad_norm": 1.0844063758850098,
      "learning_rate": 0.00011011826544021025,
      "loss": 1.6061,
      "step": 420
    },
    {
      "epoch": 0.8493827160493828,
      "grad_norm": 1.1012274026870728,
      "learning_rate": 0.00011274638633377136,
      "loss": 1.5861,
      "step": 430
    },
    {
      "epoch": 0.8691358024691358,
      "grad_norm": 1.0902862548828125,
      "learning_rate": 0.00011537450722733248,
      "loss": 1.6138,
      "step": 440
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 0.9743591547012329,
      "learning_rate": 0.00011800262812089358,
      "loss": 1.5981,
      "step": 450
    },
    {
      "epoch": 0.908641975308642,
      "grad_norm": 1.0559654235839844,
      "learning_rate": 0.00012063074901445468,
      "loss": 1.5979,
      "step": 460
    },
    {
      "epoch": 0.928395061728395,
      "grad_norm": 1.0224318504333496,
      "learning_rate": 0.00012325886990801577,
      "loss": 1.6234,
      "step": 470
    },
    {
      "epoch": 0.9481481481481482,
      "grad_norm": 1.0794756412506104,
      "learning_rate": 0.0001258869908015769,
      "loss": 1.5527,
      "step": 480
    },
    {
      "epoch": 0.9679012345679012,
      "grad_norm": 1.1146332025527954,
      "learning_rate": 0.000128515111695138,
      "loss": 1.5497,
      "step": 490
    },
    {
      "epoch": 0.9876543209876543,
      "grad_norm": 1.104157567024231,
      "learning_rate": 0.0001311432325886991,
      "loss": 1.5808,
      "step": 500
    },
    {
      "epoch": 1.005925925925926,
      "grad_norm": 1.02880859375,
      "learning_rate": 0.0001337713534822602,
      "loss": 1.5448,
      "step": 510
    },
    {
      "epoch": 1.025679012345679,
      "grad_norm": 1.0039111375808716,
      "learning_rate": 0.0001363994743758213,
      "loss": 1.5648,
      "step": 520
    },
    {
      "epoch": 1.0454320987654322,
      "grad_norm": 1.0228685140609741,
      "learning_rate": 0.0001390275952693824,
      "loss": 1.5693,
      "step": 530
    },
    {
      "epoch": 1.0651851851851852,
      "grad_norm": 0.9523122906684875,
      "learning_rate": 0.0001416557161629435,
      "loss": 1.5484,
      "step": 540
    },
    {
      "epoch": 1.0849382716049383,
      "grad_norm": 0.9736740589141846,
      "learning_rate": 0.0001442838370565046,
      "loss": 1.5735,
      "step": 550
    },
    {
      "epoch": 1.1046913580246913,
      "grad_norm": 0.9347996115684509,
      "learning_rate": 0.0001469119579500657,
      "loss": 1.551,
      "step": 560
    },
    {
      "epoch": 1.1244444444444444,
      "grad_norm": 0.9747572541236877,
      "learning_rate": 0.0001495400788436268,
      "loss": 1.5249,
      "step": 570
    },
    {
      "epoch": 1.1441975308641976,
      "grad_norm": 1.105644941329956,
      "learning_rate": 0.00015216819973718791,
      "loss": 1.5618,
      "step": 580
    },
    {
      "epoch": 1.1639506172839507,
      "grad_norm": 1.0483441352844238,
      "learning_rate": 0.00015479632063074902,
      "loss": 1.5758,
      "step": 590
    },
    {
      "epoch": 1.1837037037037037,
      "grad_norm": 1.0894588232040405,
      "learning_rate": 0.00015742444152431012,
      "loss": 1.4973,
      "step": 600
    },
    {
      "epoch": 1.2034567901234567,
      "grad_norm": 0.943345308303833,
      "learning_rate": 0.00016005256241787125,
      "loss": 1.492,
      "step": 610
    },
    {
      "epoch": 1.2232098765432098,
      "grad_norm": 0.9051336646080017,
      "learning_rate": 0.00016268068331143235,
      "loss": 1.5605,
      "step": 620
    },
    {
      "epoch": 1.242962962962963,
      "grad_norm": 0.9463896751403809,
      "learning_rate": 0.00016530880420499345,
      "loss": 1.5482,
      "step": 630
    },
    {
      "epoch": 1.262716049382716,
      "grad_norm": 0.9909905195236206,
      "learning_rate": 0.00016793692509855455,
      "loss": 1.4996,
      "step": 640
    },
    {
      "epoch": 1.2824691358024691,
      "grad_norm": 0.9302084445953369,
      "learning_rate": 0.00017056504599211565,
      "loss": 1.5432,
      "step": 650
    },
    {
      "epoch": 1.3022222222222222,
      "grad_norm": 0.9497740268707275,
      "learning_rate": 0.00017319316688567675,
      "loss": 1.5096,
      "step": 660
    },
    {
      "epoch": 1.3219753086419752,
      "grad_norm": 0.9821088314056396,
      "learning_rate": 0.00017582128777923785,
      "loss": 1.5015,
      "step": 670
    },
    {
      "epoch": 1.3417283950617285,
      "grad_norm": 0.9658571481704712,
      "learning_rate": 0.00017844940867279896,
      "loss": 1.4955,
      "step": 680
    },
    {
      "epoch": 1.3614814814814815,
      "grad_norm": 0.9455360174179077,
      "learning_rate": 0.00018107752956636006,
      "loss": 1.5351,
      "step": 690
    },
    {
      "epoch": 1.3812345679012346,
      "grad_norm": 0.9804064631462097,
      "learning_rate": 0.00018370565045992116,
      "loss": 1.5207,
      "step": 700
    },
    {
      "epoch": 1.4009876543209876,
      "grad_norm": 0.8863989114761353,
      "learning_rate": 0.00018633377135348226,
      "loss": 1.5122,
      "step": 710
    },
    {
      "epoch": 1.4207407407407406,
      "grad_norm": 0.8953720927238464,
      "learning_rate": 0.00018896189224704336,
      "loss": 1.4479,
      "step": 720
    },
    {
      "epoch": 1.440493827160494,
      "grad_norm": 0.877299427986145,
      "learning_rate": 0.00019159001314060446,
      "loss": 1.4746,
      "step": 730
    },
    {
      "epoch": 1.460246913580247,
      "grad_norm": 0.8663986325263977,
      "learning_rate": 0.00019421813403416556,
      "loss": 1.4801,
      "step": 740
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.9532702565193176,
      "learning_rate": 0.0001968462549277267,
      "loss": 1.5002,
      "step": 750
    },
    {
      "epoch": 1.499753086419753,
      "grad_norm": 0.9898898601531982,
      "learning_rate": 0.0001994743758212878,
      "loss": 1.4738,
      "step": 760
    },
    {
      "epoch": 1.519506172839506,
      "grad_norm": 0.8750928640365601,
      "learning_rate": 0.00019999829903450295,
      "loss": 1.4953,
      "step": 770
    },
    {
      "epoch": 1.5392592592592593,
      "grad_norm": 0.8254457116127014,
      "learning_rate": 0.00019999138896134518,
      "loss": 1.4843,
      "step": 780
    },
    {
      "epoch": 1.5590123456790124,
      "grad_norm": 0.8605950474739075,
      "learning_rate": 0.00019997916383720604,
      "loss": 1.4954,
      "step": 790
    },
    {
      "epoch": 1.5787654320987654,
      "grad_norm": 0.785704493522644,
      "learning_rate": 0.0001999616243119131,
      "loss": 1.4783,
      "step": 800
    },
    {
      "epoch": 1.5985185185185187,
      "grad_norm": 0.8783153891563416,
      "learning_rate": 0.0001999387713177814,
      "loss": 1.4378,
      "step": 810
    },
    {
      "epoch": 1.6182716049382715,
      "grad_norm": 0.7940098643302917,
      "learning_rate": 0.0001999106060695638,
      "loss": 1.482,
      "step": 820
    },
    {
      "epoch": 1.6380246913580248,
      "grad_norm": 0.8054230809211731,
      "learning_rate": 0.00019987713006438648,
      "loss": 1.477,
      "step": 830
    },
    {
      "epoch": 1.6577777777777778,
      "grad_norm": 0.8274779915809631,
      "learning_rate": 0.00019983834508166946,
      "loss": 1.4473,
      "step": 840
    },
    {
      "epoch": 1.6775308641975308,
      "grad_norm": 0.8069911003112793,
      "learning_rate": 0.00019979425318303188,
      "loss": 1.4833,
      "step": 850
    },
    {
      "epoch": 1.697283950617284,
      "grad_norm": 0.8396931886672974,
      "learning_rate": 0.0001997448567121824,
      "loss": 1.471,
      "step": 860
    },
    {
      "epoch": 1.717037037037037,
      "grad_norm": 0.7993788719177246,
      "learning_rate": 0.00019969015829479476,
      "loss": 1.4695,
      "step": 870
    },
    {
      "epoch": 1.7367901234567902,
      "grad_norm": 0.7766834497451782,
      "learning_rate": 0.00019963016083836808,
      "loss": 1.4596,
      "step": 880
    },
    {
      "epoch": 1.7565432098765432,
      "grad_norm": 0.8045221567153931,
      "learning_rate": 0.00019956486753207239,
      "loss": 1.4411,
      "step": 890
    },
    {
      "epoch": 1.7762962962962963,
      "grad_norm": 0.8297169208526611,
      "learning_rate": 0.00019949428184657905,
      "loss": 1.4584,
      "step": 900
    },
    {
      "epoch": 1.7960493827160495,
      "grad_norm": 0.8501317501068115,
      "learning_rate": 0.00019941840753387634,
      "loss": 1.4571,
      "step": 910
    },
    {
      "epoch": 1.8158024691358023,
      "grad_norm": 0.8010051846504211,
      "learning_rate": 0.00019933724862706992,
      "loss": 1.4702,
      "step": 920
    },
    {
      "epoch": 1.8355555555555556,
      "grad_norm": 0.7976624369621277,
      "learning_rate": 0.00019925080944016856,
      "loss": 1.4681,
      "step": 930
    },
    {
      "epoch": 1.8553086419753086,
      "grad_norm": 0.8253341317176819,
      "learning_rate": 0.0001991590945678548,
      "loss": 1.4646,
      "step": 940
    },
    {
      "epoch": 1.8750617283950617,
      "grad_norm": 0.772279679775238,
      "learning_rate": 0.0001990621088852406,
      "loss": 1.4125,
      "step": 950
    },
    {
      "epoch": 1.894814814814815,
      "grad_norm": 0.777018129825592,
      "learning_rate": 0.0001989598575476084,
      "loss": 1.4575,
      "step": 960
    },
    {
      "epoch": 1.9145679012345678,
      "grad_norm": 0.8258969783782959,
      "learning_rate": 0.00019885234599013694,
      "loss": 1.4634,
      "step": 970
    },
    {
      "epoch": 1.934320987654321,
      "grad_norm": 0.8080457448959351,
      "learning_rate": 0.00019873957992761238,
      "loss": 1.441,
      "step": 980
    },
    {
      "epoch": 1.954074074074074,
      "grad_norm": 0.7684703469276428,
      "learning_rate": 0.0001986215653541246,
      "loss": 1.4134,
      "step": 990
    },
    {
      "epoch": 1.9738271604938271,
      "grad_norm": 0.7662165760993958,
      "learning_rate": 0.00019849830854274858,
      "loss": 1.4406,
      "step": 1000
    },
    {
      "epoch": 1.9935802469135804,
      "grad_norm": 0.7816095948219299,
      "learning_rate": 0.00019836981604521076,
      "loss": 1.4125,
      "step": 1010
    },
    {
      "epoch": 2.011851851851852,
      "grad_norm": 0.7590295672416687,
      "learning_rate": 0.0001982360946915411,
      "loss": 1.4304,
      "step": 1020
    },
    {
      "epoch": 2.031604938271605,
      "grad_norm": 0.774787187576294,
      "learning_rate": 0.00019809715158970978,
      "loss": 1.386,
      "step": 1030
    },
    {
      "epoch": 2.051358024691358,
      "grad_norm": 0.775346577167511,
      "learning_rate": 0.00019795299412524945,
      "loss": 1.4248,
      "step": 1040
    },
    {
      "epoch": 2.071111111111111,
      "grad_norm": 0.7533090710639954,
      "learning_rate": 0.00019780362996086267,
      "loss": 1.3872,
      "step": 1050
    },
    {
      "epoch": 2.0908641975308644,
      "grad_norm": 0.8076508641242981,
      "learning_rate": 0.00019764906703601464,
      "loss": 1.4062,
      "step": 1060
    },
    {
      "epoch": 2.110617283950617,
      "grad_norm": 0.8141077756881714,
      "learning_rate": 0.00019748931356651105,
      "loss": 1.3973,
      "step": 1070
    },
    {
      "epoch": 2.1303703703703705,
      "grad_norm": 0.792428731918335,
      "learning_rate": 0.00019732437804406144,
      "loss": 1.3933,
      "step": 1080
    },
    {
      "epoch": 2.1501234567901233,
      "grad_norm": 0.7866876721382141,
      "learning_rate": 0.00019715426923582792,
      "loss": 1.3986,
      "step": 1090
    },
    {
      "epoch": 2.1698765432098766,
      "grad_norm": 0.748201847076416,
      "learning_rate": 0.00019697899618395894,
      "loss": 1.3698,
      "step": 1100
    },
    {
      "epoch": 2.18962962962963,
      "grad_norm": 0.7232767343521118,
      "learning_rate": 0.00019679856820510884,
      "loss": 1.3977,
      "step": 1110
    },
    {
      "epoch": 2.2093827160493826,
      "grad_norm": 0.771785318851471,
      "learning_rate": 0.00019661299488994257,
      "loss": 1.4055,
      "step": 1120
    },
    {
      "epoch": 2.229135802469136,
      "grad_norm": 0.7382349967956543,
      "learning_rate": 0.00019642228610262583,
      "loss": 1.3937,
      "step": 1130
    },
    {
      "epoch": 2.2488888888888887,
      "grad_norm": 0.7946950793266296,
      "learning_rate": 0.00019622645198030076,
      "loss": 1.4103,
      "step": 1140
    },
    {
      "epoch": 2.268641975308642,
      "grad_norm": 0.7422038316726685,
      "learning_rate": 0.00019602550293254723,
      "loss": 1.3991,
      "step": 1150
    },
    {
      "epoch": 2.2883950617283952,
      "grad_norm": 0.7992005348205566,
      "learning_rate": 0.00019581944964082933,
      "loss": 1.4217,
      "step": 1160
    },
    {
      "epoch": 2.308148148148148,
      "grad_norm": 0.7522461414337158,
      "learning_rate": 0.00019560830305792774,
      "loss": 1.4026,
      "step": 1170
    },
    {
      "epoch": 2.3279012345679013,
      "grad_norm": 0.766916811466217,
      "learning_rate": 0.00019539207440735743,
      "loss": 1.393,
      "step": 1180
    },
    {
      "epoch": 2.347654320987654,
      "grad_norm": 0.7769410610198975,
      "learning_rate": 0.00019517077518277113,
      "loss": 1.3669,
      "step": 1190
    },
    {
      "epoch": 2.3674074074074074,
      "grad_norm": 0.746833860874176,
      "learning_rate": 0.00019494441714734836,
      "loss": 1.3513,
      "step": 1200
    },
    {
      "epoch": 2.3871604938271607,
      "grad_norm": 0.711819589138031,
      "learning_rate": 0.00019471301233317024,
      "loss": 1.3847,
      "step": 1210
    },
    {
      "epoch": 2.4069135802469135,
      "grad_norm": 0.722344160079956,
      "learning_rate": 0.0001944765730405798,
      "loss": 1.3821,
      "step": 1220
    },
    {
      "epoch": 2.4266666666666667,
      "grad_norm": 0.7364143133163452,
      "learning_rate": 0.00019423511183752816,
      "loss": 1.4316,
      "step": 1230
    },
    {
      "epoch": 2.4464197530864196,
      "grad_norm": 0.7609429955482483,
      "learning_rate": 0.00019398864155890659,
      "loss": 1.4114,
      "step": 1240
    },
    {
      "epoch": 2.466172839506173,
      "grad_norm": 0.7393321990966797,
      "learning_rate": 0.00019373717530586426,
      "loss": 1.3806,
      "step": 1250
    },
    {
      "epoch": 2.485925925925926,
      "grad_norm": 0.7334694266319275,
      "learning_rate": 0.0001934807264451117,
      "loss": 1.3823,
      "step": 1260
    },
    {
      "epoch": 2.505679012345679,
      "grad_norm": 0.7223696708679199,
      "learning_rate": 0.0001932193086082104,
      "loss": 1.3561,
      "step": 1270
    },
    {
      "epoch": 2.525432098765432,
      "grad_norm": 0.7505025267601013,
      "learning_rate": 0.0001929529356908482,
      "loss": 1.4123,
      "step": 1280
    },
    {
      "epoch": 2.5451851851851854,
      "grad_norm": 0.7639083862304688,
      "learning_rate": 0.00019268162185210076,
      "loss": 1.3372,
      "step": 1290
    },
    {
      "epoch": 2.5649382716049383,
      "grad_norm": 0.7099962830543518,
      "learning_rate": 0.00019240538151367882,
      "loss": 1.3495,
      "step": 1300
    },
    {
      "epoch": 2.5846913580246915,
      "grad_norm": 0.755066454410553,
      "learning_rate": 0.0001921242293591615,
      "loss": 1.3594,
      "step": 1310
    },
    {
      "epoch": 2.6044444444444443,
      "grad_norm": 0.760964035987854,
      "learning_rate": 0.00019183818033321614,
      "loss": 1.3918,
      "step": 1320
    },
    {
      "epoch": 2.6241975308641976,
      "grad_norm": 0.7463734149932861,
      "learning_rate": 0.0001915472496408035,
      "loss": 1.3721,
      "step": 1330
    },
    {
      "epoch": 2.6439506172839504,
      "grad_norm": 0.7203443646430969,
      "learning_rate": 0.00019125145274636996,
      "loss": 1.3686,
      "step": 1340
    },
    {
      "epoch": 2.6637037037037037,
      "grad_norm": 0.724783718585968,
      "learning_rate": 0.0001909508053730251,
      "loss": 1.3654,
      "step": 1350
    },
    {
      "epoch": 2.683456790123457,
      "grad_norm": 0.7702601552009583,
      "learning_rate": 0.00019064532350170627,
      "loss": 1.3565,
      "step": 1360
    },
    {
      "epoch": 2.7032098765432098,
      "grad_norm": 0.7267993092536926,
      "learning_rate": 0.00019033502337032895,
      "loss": 1.3938,
      "step": 1370
    },
    {
      "epoch": 2.722962962962963,
      "grad_norm": 0.725667417049408,
      "learning_rate": 0.0001900199214729235,
      "loss": 1.3764,
      "step": 1380
    },
    {
      "epoch": 2.7427160493827163,
      "grad_norm": 0.7308797836303711,
      "learning_rate": 0.00018970003455875877,
      "loss": 1.3522,
      "step": 1390
    },
    {
      "epoch": 2.762469135802469,
      "grad_norm": 0.7074142098426819,
      "learning_rate": 0.00018937537963145155,
      "loss": 1.3783,
      "step": 1400
    },
    {
      "epoch": 2.7822222222222224,
      "grad_norm": 0.700568675994873,
      "learning_rate": 0.00018904597394806273,
      "loss": 1.3712,
      "step": 1410
    },
    {
      "epoch": 2.801975308641975,
      "grad_norm": 0.7205440402030945,
      "learning_rate": 0.00018871183501818005,
      "loss": 1.3895,
      "step": 1420
    },
    {
      "epoch": 2.8217283950617285,
      "grad_norm": 0.7255626320838928,
      "learning_rate": 0.00018837298060298742,
      "loss": 1.3547,
      "step": 1430
    },
    {
      "epoch": 2.8414814814814813,
      "grad_norm": 0.7061998844146729,
      "learning_rate": 0.00018802942871432076,
      "loss": 1.4031,
      "step": 1440
    },
    {
      "epoch": 2.8612345679012345,
      "grad_norm": 0.7350696921348572,
      "learning_rate": 0.00018768119761371064,
      "loss": 1.3559,
      "step": 1450
    },
    {
      "epoch": 2.880987654320988,
      "grad_norm": 0.7250066995620728,
      "learning_rate": 0.00018732830581141147,
      "loss": 1.3454,
      "step": 1460
    },
    {
      "epoch": 2.9007407407407406,
      "grad_norm": 0.7536110281944275,
      "learning_rate": 0.00018697077206541774,
      "loss": 1.3697,
      "step": 1470
    },
    {
      "epoch": 2.920493827160494,
      "grad_norm": 0.7262146472930908,
      "learning_rate": 0.00018660861538046683,
      "loss": 1.3668,
      "step": 1480
    },
    {
      "epoch": 2.940246913580247,
      "grad_norm": 0.7313677072525024,
      "learning_rate": 0.0001862418550070288,
      "loss": 1.3659,
      "step": 1490
    },
    {
      "epoch": 2.96,
      "grad_norm": 0.6764166355133057,
      "learning_rate": 0.00018587051044028328,
      "loss": 1.3973,
      "step": 1500
    },
    {
      "epoch": 2.9797530864197532,
      "grad_norm": 0.7077234387397766,
      "learning_rate": 0.00018549460141908296,
      "loss": 1.3497,
      "step": 1510
    },
    {
      "epoch": 2.999506172839506,
      "grad_norm": 0.744873046875,
      "learning_rate": 0.00018511414792490464,
      "loss": 1.4061,
      "step": 1520
    },
    {
      "epoch": 3.017777777777778,
      "grad_norm": 0.7191585302352905,
      "learning_rate": 0.00018472917018078685,
      "loss": 1.3469,
      "step": 1530
    },
    {
      "epoch": 3.0375308641975307,
      "grad_norm": 0.6965366005897522,
      "learning_rate": 0.00018433968865025512,
      "loss": 1.3105,
      "step": 1540
    },
    {
      "epoch": 3.057283950617284,
      "grad_norm": 0.7087466716766357,
      "learning_rate": 0.0001839457240362341,
      "loss": 1.3453,
      "step": 1550
    },
    {
      "epoch": 3.0770370370370372,
      "grad_norm": 0.746351420879364,
      "learning_rate": 0.0001835472972799471,
      "loss": 1.2863,
      "step": 1560
    },
    {
      "epoch": 3.09679012345679,
      "grad_norm": 0.7188896536827087,
      "learning_rate": 0.00018314442955980303,
      "loss": 1.3224,
      "step": 1570
    },
    {
      "epoch": 3.1165432098765433,
      "grad_norm": 0.7166439294815063,
      "learning_rate": 0.00018273714229027067,
      "loss": 1.2983,
      "step": 1580
    },
    {
      "epoch": 3.136296296296296,
      "grad_norm": 0.7173548340797424,
      "learning_rate": 0.00018232545712074013,
      "loss": 1.3166,
      "step": 1590
    },
    {
      "epoch": 3.1560493827160494,
      "grad_norm": 0.7240375280380249,
      "learning_rate": 0.00018190939593437256,
      "loss": 1.3489,
      "step": 1600
    },
    {
      "epoch": 3.1758024691358027,
      "grad_norm": 0.6867201924324036,
      "learning_rate": 0.0001814889808469365,
      "loss": 1.3252,
      "step": 1610
    },
    {
      "epoch": 3.1955555555555555,
      "grad_norm": 0.724376380443573,
      "learning_rate": 0.00018106423420563247,
      "loss": 1.3173,
      "step": 1620
    },
    {
      "epoch": 3.2153086419753087,
      "grad_norm": 0.6975104808807373,
      "learning_rate": 0.00018063517858790516,
      "loss": 1.3093,
      "step": 1630
    },
    {
      "epoch": 3.2350617283950616,
      "grad_norm": 0.729989230632782,
      "learning_rate": 0.00018020183680024325,
      "loss": 1.3531,
      "step": 1640
    },
    {
      "epoch": 3.254814814814815,
      "grad_norm": 0.7386916875839233,
      "learning_rate": 0.00017976423187696718,
      "loss": 1.3155,
      "step": 1650
    },
    {
      "epoch": 3.274567901234568,
      "grad_norm": 0.7166791558265686,
      "learning_rate": 0.0001793223870790048,
      "loss": 1.3329,
      "step": 1660
    },
    {
      "epoch": 3.294320987654321,
      "grad_norm": 0.7261766195297241,
      "learning_rate": 0.00017887632589265467,
      "loss": 1.3285,
      "step": 1670
    },
    {
      "epoch": 3.314074074074074,
      "grad_norm": 0.7356224060058594,
      "learning_rate": 0.00017842607202833805,
      "loss": 1.3236,
      "step": 1680
    },
    {
      "epoch": 3.333827160493827,
      "grad_norm": 0.7504083514213562,
      "learning_rate": 0.0001779716494193383,
      "loss": 1.3456,
      "step": 1690
    },
    {
      "epoch": 3.3535802469135803,
      "grad_norm": 0.7207203507423401,
      "learning_rate": 0.00017751308222052864,
      "loss": 1.3465,
      "step": 1700
    },
    {
      "epoch": 3.3733333333333335,
      "grad_norm": 0.7264026999473572,
      "learning_rate": 0.00017705039480708846,
      "loss": 1.353,
      "step": 1710
    },
    {
      "epoch": 3.3930864197530863,
      "grad_norm": 0.7085683345794678,
      "learning_rate": 0.0001765836117732076,
      "loss": 1.3144,
      "step": 1720
    },
    {
      "epoch": 3.4128395061728396,
      "grad_norm": 0.7169286012649536,
      "learning_rate": 0.00017611275793077868,
      "loss": 1.3453,
      "step": 1730
    },
    {
      "epoch": 3.4325925925925924,
      "grad_norm": 0.7082623243331909,
      "learning_rate": 0.0001756378583080788,
      "loss": 1.2999,
      "step": 1740
    },
    {
      "epoch": 3.4523456790123457,
      "grad_norm": 0.7157137393951416,
      "learning_rate": 0.00017515893814843865,
      "loss": 1.3028,
      "step": 1750
    },
    {
      "epoch": 3.472098765432099,
      "grad_norm": 0.7108745574951172,
      "learning_rate": 0.000174676022908901,
      "loss": 1.3136,
      "step": 1760
    },
    {
      "epoch": 3.4918518518518518,
      "grad_norm": 0.7159055471420288,
      "learning_rate": 0.00017418913825886746,
      "loss": 1.3474,
      "step": 1770
    },
    {
      "epoch": 3.511604938271605,
      "grad_norm": 0.7095149159431458,
      "learning_rate": 0.00017369831007873395,
      "loss": 1.3313,
      "step": 1780
    },
    {
      "epoch": 3.5313580246913583,
      "grad_norm": 0.7023637294769287,
      "learning_rate": 0.0001732035644585151,
      "loss": 1.3014,
      "step": 1790
    },
    {
      "epoch": 3.551111111111111,
      "grad_norm": 0.7034569382667542,
      "learning_rate": 0.00017270492769645736,
      "loss": 1.3095,
      "step": 1800
    },
    {
      "epoch": 3.570864197530864,
      "grad_norm": 0.7332428693771362,
      "learning_rate": 0.0001722024262976413,
      "loss": 1.308,
      "step": 1810
    },
    {
      "epoch": 3.590617283950617,
      "grad_norm": 0.7463644742965698,
      "learning_rate": 0.00017169608697257242,
      "loss": 1.3377,
      "step": 1820
    },
    {
      "epoch": 3.6103703703703705,
      "grad_norm": 0.7115930914878845,
      "learning_rate": 0.00017118593663576168,
      "loss": 1.3445,
      "step": 1830
    },
    {
      "epoch": 3.6301234567901233,
      "grad_norm": 0.7064332365989685,
      "learning_rate": 0.00017067200240429456,
      "loss": 1.307,
      "step": 1840
    },
    {
      "epoch": 3.6498765432098765,
      "grad_norm": 0.706017255783081,
      "learning_rate": 0.00017015431159638988,
      "loss": 1.3178,
      "step": 1850
    },
    {
      "epoch": 3.66962962962963,
      "grad_norm": 0.7075929045677185,
      "learning_rate": 0.00016963289172994758,
      "loss": 1.3046,
      "step": 1860
    },
    {
      "epoch": 3.6893827160493826,
      "grad_norm": 0.7169323563575745,
      "learning_rate": 0.00016910777052108603,
      "loss": 1.3199,
      "step": 1870
    },
    {
      "epoch": 3.709135802469136,
      "grad_norm": 0.6983360648155212,
      "learning_rate": 0.0001685789758826688,
      "loss": 1.3416,
      "step": 1880
    },
    {
      "epoch": 3.728888888888889,
      "grad_norm": 0.733479917049408,
      "learning_rate": 0.0001680465359228209,
      "loss": 1.3262,
      "step": 1890
    },
    {
      "epoch": 3.748641975308642,
      "grad_norm": 0.7096752524375916,
      "learning_rate": 0.0001675104789434347,
      "loss": 1.3036,
      "step": 1900
    },
    {
      "epoch": 3.7683950617283952,
      "grad_norm": 0.7516841292381287,
      "learning_rate": 0.0001669708334386656,
      "loss": 1.3084,
      "step": 1910
    },
    {
      "epoch": 3.788148148148148,
      "grad_norm": 0.6857229471206665,
      "learning_rate": 0.00016642762809341744,
      "loss": 1.3016,
      "step": 1920
    },
    {
      "epoch": 3.8079012345679013,
      "grad_norm": 0.6839893460273743,
      "learning_rate": 0.00016588089178181753,
      "loss": 1.3501,
      "step": 1930
    },
    {
      "epoch": 3.827654320987654,
      "grad_norm": 0.6930967569351196,
      "learning_rate": 0.00016533065356568206,
      "loss": 1.294,
      "step": 1940
    },
    {
      "epoch": 3.8474074074074074,
      "grad_norm": 0.7049363255500793,
      "learning_rate": 0.0001647769426929714,
      "loss": 1.3217,
      "step": 1950
    },
    {
      "epoch": 3.8671604938271606,
      "grad_norm": 0.6790988445281982,
      "learning_rate": 0.0001642197885962351,
      "loss": 1.3106,
      "step": 1960
    },
    {
      "epoch": 3.8869135802469135,
      "grad_norm": 0.7323857545852661,
      "learning_rate": 0.00016365922089104764,
      "loss": 1.3394,
      "step": 1970
    },
    {
      "epoch": 3.9066666666666667,
      "grad_norm": 0.6923937201499939,
      "learning_rate": 0.00016309526937443423,
      "loss": 1.3234,
      "step": 1980
    },
    {
      "epoch": 3.92641975308642,
      "grad_norm": 0.7149975895881653,
      "learning_rate": 0.00016252796402328672,
      "loss": 1.3021,
      "step": 1990
    },
    {
      "epoch": 3.946172839506173,
      "grad_norm": 0.693134605884552,
      "learning_rate": 0.0001619573349927705,
      "loss": 1.2827,
      "step": 2000
    },
    {
      "epoch": 3.965925925925926,
      "grad_norm": 0.6898493766784668,
      "learning_rate": 0.00016138341261472133,
      "loss": 1.2875,
      "step": 2010
    },
    {
      "epoch": 3.985679012345679,
      "grad_norm": 0.7259241938591003,
      "learning_rate": 0.00016080622739603312,
      "loss": 1.32,
      "step": 2020
    },
    {
      "epoch": 4.003950617283951,
      "grad_norm": 0.6922501921653748,
      "learning_rate": 0.00016022581001703644,
      "loss": 1.3007,
      "step": 2030
    },
    {
      "epoch": 4.023703703703704,
      "grad_norm": 0.6925665140151978,
      "learning_rate": 0.00015964219132986752,
      "loss": 1.2869,
      "step": 2040
    },
    {
      "epoch": 4.043456790123456,
      "grad_norm": 0.7237018942832947,
      "learning_rate": 0.00015905540235682853,
      "loss": 1.2965,
      "step": 2050
    },
    {
      "epoch": 4.06320987654321,
      "grad_norm": 0.709386944770813,
      "learning_rate": 0.00015846547428873838,
      "loss": 1.2717,
      "step": 2060
    },
    {
      "epoch": 4.082962962962963,
      "grad_norm": 0.7499399781227112,
      "learning_rate": 0.0001578724384832748,
      "loss": 1.2502,
      "step": 2070
    },
    {
      "epoch": 4.102716049382716,
      "grad_norm": 0.729556679725647,
      "learning_rate": 0.00015727632646330767,
      "loss": 1.2668,
      "step": 2080
    },
    {
      "epoch": 4.122469135802469,
      "grad_norm": 0.7367806434631348,
      "learning_rate": 0.0001566771699152233,
      "loss": 1.2498,
      "step": 2090
    },
    {
      "epoch": 4.142222222222222,
      "grad_norm": 0.725724995136261,
      "learning_rate": 0.00015607500068724015,
      "loss": 1.2893,
      "step": 2100
    },
    {
      "epoch": 4.161975308641975,
      "grad_norm": 0.6855913996696472,
      "learning_rate": 0.0001554698507877159,
      "loss": 1.2521,
      "step": 2110
    },
    {
      "epoch": 4.181728395061729,
      "grad_norm": 0.7039944529533386,
      "learning_rate": 0.00015486175238344613,
      "loss": 1.2893,
      "step": 2120
    },
    {
      "epoch": 4.201481481481482,
      "grad_norm": 0.7112748622894287,
      "learning_rate": 0.00015425073779795447,
      "loss": 1.3168,
      "step": 2130
    },
    {
      "epoch": 4.221234567901234,
      "grad_norm": 0.6964794993400574,
      "learning_rate": 0.0001536368395097743,
      "loss": 1.282,
      "step": 2140
    },
    {
      "epoch": 4.240987654320987,
      "grad_norm": 0.7286403775215149,
      "learning_rate": 0.00015302009015072266,
      "loss": 1.2957,
      "step": 2150
    },
    {
      "epoch": 4.260740740740741,
      "grad_norm": 0.723578691482544,
      "learning_rate": 0.00015240052250416528,
      "loss": 1.292,
      "step": 2160
    },
    {
      "epoch": 4.280493827160494,
      "grad_norm": 0.7308707237243652,
      "learning_rate": 0.0001517781695032744,
      "loss": 1.2678,
      "step": 2170
    },
    {
      "epoch": 4.300246913580247,
      "grad_norm": 0.7217671275138855,
      "learning_rate": 0.00015115306422927794,
      "loss": 1.2816,
      "step": 2180
    },
    {
      "epoch": 4.32,
      "grad_norm": 0.7318040132522583,
      "learning_rate": 0.0001505252399097011,
      "loss": 1.2463,
      "step": 2190
    },
    {
      "epoch": 4.339753086419753,
      "grad_norm": 0.7070186138153076,
      "learning_rate": 0.00014989472991660025,
      "loss": 1.2649,
      "step": 2200
    },
    {
      "epoch": 4.359506172839506,
      "grad_norm": 0.7181482315063477,
      "learning_rate": 0.0001492615677647889,
      "loss": 1.2907,
      "step": 2210
    },
    {
      "epoch": 4.37925925925926,
      "grad_norm": 0.722278356552124,
      "learning_rate": 0.00014862578711005638,
      "loss": 1.2556,
      "step": 2220
    },
    {
      "epoch": 4.3990123456790124,
      "grad_norm": 0.7314203977584839,
      "learning_rate": 0.00014798742174737867,
      "loss": 1.2699,
      "step": 2230
    },
    {
      "epoch": 4.418765432098765,
      "grad_norm": 0.6920257806777954,
      "learning_rate": 0.00014734650560912222,
      "loss": 1.2841,
      "step": 2240
    },
    {
      "epoch": 4.438518518518519,
      "grad_norm": 0.6988356113433838,
      "learning_rate": 0.0001467030727632401,
      "loss": 1.2778,
      "step": 2250
    },
    {
      "epoch": 4.458271604938272,
      "grad_norm": 0.7028444409370422,
      "learning_rate": 0.00014605715741146117,
      "loss": 1.2617,
      "step": 2260
    },
    {
      "epoch": 4.478024691358025,
      "grad_norm": 0.6982978582382202,
      "learning_rate": 0.00014540879388747228,
      "loss": 1.2774,
      "step": 2270
    },
    {
      "epoch": 4.497777777777777,
      "grad_norm": 0.7154090404510498,
      "learning_rate": 0.00014475801665509284,
      "loss": 1.2678,
      "step": 2280
    },
    {
      "epoch": 4.517530864197531,
      "grad_norm": 0.704102635383606,
      "learning_rate": 0.0001441048603064434,
      "loss": 1.2689,
      "step": 2290
    },
    {
      "epoch": 4.537283950617284,
      "grad_norm": 0.7012678980827332,
      "learning_rate": 0.00014344935956010638,
      "loss": 1.2861,
      "step": 2300
    },
    {
      "epoch": 4.557037037037037,
      "grad_norm": 0.7365461587905884,
      "learning_rate": 0.0001427915492592811,
      "loss": 1.2891,
      "step": 2310
    },
    {
      "epoch": 4.5767901234567905,
      "grad_norm": 0.7376147508621216,
      "learning_rate": 0.00014213146436993127,
      "loss": 1.2655,
      "step": 2320
    },
    {
      "epoch": 4.596543209876543,
      "grad_norm": 0.7058185338973999,
      "learning_rate": 0.00014146913997892664,
      "loss": 1.2881,
      "step": 2330
    },
    {
      "epoch": 4.616296296296296,
      "grad_norm": 0.7596071362495422,
      "learning_rate": 0.00014080461129217786,
      "loss": 1.2792,
      "step": 2340
    },
    {
      "epoch": 4.636049382716049,
      "grad_norm": 0.7025811672210693,
      "learning_rate": 0.0001401379136327651,
      "loss": 1.2619,
      "step": 2350
    },
    {
      "epoch": 4.655802469135803,
      "grad_norm": 0.7408230900764465,
      "learning_rate": 0.00013946908243906044,
      "loss": 1.2817,
      "step": 2360
    },
    {
      "epoch": 4.6755555555555555,
      "grad_norm": 0.7333143353462219,
      "learning_rate": 0.0001387981532628442,
      "loss": 1.2709,
      "step": 2370
    },
    {
      "epoch": 4.695308641975308,
      "grad_norm": 0.7171742916107178,
      "learning_rate": 0.00013812516176741512,
      "loss": 1.3022,
      "step": 2380
    },
    {
      "epoch": 4.715061728395062,
      "grad_norm": 0.7192667126655579,
      "learning_rate": 0.0001374501437256947,
      "loss": 1.3023,
      "step": 2390
    },
    {
      "epoch": 4.734814814814815,
      "grad_norm": 0.7015146017074585,
      "learning_rate": 0.0001367731350183257,
      "loss": 1.2635,
      "step": 2400
    },
    {
      "epoch": 4.754567901234568,
      "grad_norm": 0.7077828049659729,
      "learning_rate": 0.0001360941716317649,
      "loss": 1.251,
      "step": 2410
    },
    {
      "epoch": 4.774320987654321,
      "grad_norm": 0.7122102975845337,
      "learning_rate": 0.0001354132896563701,
      "loss": 1.2733,
      "step": 2420
    },
    {
      "epoch": 4.794074074074074,
      "grad_norm": 0.6942643523216248,
      "learning_rate": 0.00013473052528448201,
      "loss": 1.2505,
      "step": 2430
    },
    {
      "epoch": 4.813827160493827,
      "grad_norm": 0.7413410544395447,
      "learning_rate": 0.00013404591480850028,
      "loss": 1.2822,
      "step": 2440
    },
    {
      "epoch": 4.833580246913581,
      "grad_norm": 0.7455625534057617,
      "learning_rate": 0.00013335949461895424,
      "loss": 1.3015,
      "step": 2450
    },
    {
      "epoch": 4.8533333333333335,
      "grad_norm": 0.7315553426742554,
      "learning_rate": 0.00013267130120256889,
      "loss": 1.2979,
      "step": 2460
    },
    {
      "epoch": 4.873086419753086,
      "grad_norm": 0.6746072173118591,
      "learning_rate": 0.00013198137114032508,
      "loss": 1.2474,
      "step": 2470
    },
    {
      "epoch": 4.892839506172839,
      "grad_norm": 0.742560625076294,
      "learning_rate": 0.00013128974110551532,
      "loss": 1.2752,
      "step": 2480
    },
    {
      "epoch": 4.912592592592593,
      "grad_norm": 0.7110349535942078,
      "learning_rate": 0.00013059644786179436,
      "loss": 1.2653,
      "step": 2490
    },
    {
      "epoch": 4.932345679012346,
      "grad_norm": 0.7401759028434753,
      "learning_rate": 0.0001299015282612248,
      "loss": 1.3139,
      "step": 2500
    },
    {
      "epoch": 4.9520987654320985,
      "grad_norm": 0.7388781309127808,
      "learning_rate": 0.0001292050192423186,
      "loss": 1.2659,
      "step": 2510
    },
    {
      "epoch": 4.971851851851852,
      "grad_norm": 0.7241548895835876,
      "learning_rate": 0.00012850695782807317,
      "loss": 1.2991,
      "step": 2520
    },
    {
      "epoch": 4.991604938271605,
      "grad_norm": 0.7272842526435852,
      "learning_rate": 0.00012780738112400374,
      "loss": 1.2507,
      "step": 2530
    },
    {
      "epoch": 5.009876543209876,
      "grad_norm": 0.6918976902961731,
      "learning_rate": 0.00012710632631617093,
      "loss": 1.2885,
      "step": 2540
    },
    {
      "epoch": 5.029629629629629,
      "grad_norm": 0.7188644409179688,
      "learning_rate": 0.00012640383066920402,
      "loss": 1.2291,
      "step": 2550
    },
    {
      "epoch": 5.049382716049383,
      "grad_norm": 0.7075225710868835,
      "learning_rate": 0.00012569993152432028,
      "loss": 1.2422,
      "step": 2560
    },
    {
      "epoch": 5.069135802469136,
      "grad_norm": 0.6926276087760925,
      "learning_rate": 0.00012499466629734002,
      "loss": 1.2206,
      "step": 2570
    },
    {
      "epoch": 5.088888888888889,
      "grad_norm": 0.7162769436836243,
      "learning_rate": 0.0001242880724766977,
      "loss": 1.2414,
      "step": 2580
    },
    {
      "epoch": 5.108641975308642,
      "grad_norm": 0.7500464916229248,
      "learning_rate": 0.00012358018762144942,
      "loss": 1.2504,
      "step": 2590
    },
    {
      "epoch": 5.128395061728395,
      "grad_norm": 0.727997362613678,
      "learning_rate": 0.00012287104935927617,
      "loss": 1.2298,
      "step": 2600
    },
    {
      "epoch": 5.148148148148148,
      "grad_norm": 0.7449918389320374,
      "learning_rate": 0.000122160695384484,
      "loss": 1.2644,
      "step": 2610
    },
    {
      "epoch": 5.167901234567902,
      "grad_norm": 0.729402482509613,
      "learning_rate": 0.00012144916345600033,
      "loss": 1.2477,
      "step": 2620
    },
    {
      "epoch": 5.187654320987654,
      "grad_norm": 0.7557295560836792,
      "learning_rate": 0.00012073649139536666,
      "loss": 1.2301,
      "step": 2630
    },
    {
      "epoch": 5.207407407407407,
      "grad_norm": 0.7494889497756958,
      "learning_rate": 0.00012002271708472845,
      "loss": 1.2247,
      "step": 2640
    },
    {
      "epoch": 5.22716049382716,
      "grad_norm": 0.7241042852401733,
      "learning_rate": 0.00011930787846482123,
      "loss": 1.2531,
      "step": 2650
    },
    {
      "epoch": 5.246913580246914,
      "grad_norm": 0.7308045625686646,
      "learning_rate": 0.00011859201353295406,
      "loss": 1.2188,
      "step": 2660
    },
    {
      "epoch": 5.266666666666667,
      "grad_norm": 0.6988502740859985,
      "learning_rate": 0.00011787516034098973,
      "loss": 1.2597,
      "step": 2670
    },
    {
      "epoch": 5.286419753086419,
      "grad_norm": 0.7398092746734619,
      "learning_rate": 0.00011715735699332198,
      "loss": 1.2655,
      "step": 2680
    },
    {
      "epoch": 5.306172839506173,
      "grad_norm": 0.7594043612480164,
      "learning_rate": 0.00011643864164485027,
      "loss": 1.2596,
      "step": 2690
    },
    {
      "epoch": 5.325925925925926,
      "grad_norm": 0.7841827869415283,
      "learning_rate": 0.00011571905249895146,
      "loss": 1.2611,
      "step": 2700
    },
    {
      "epoch": 5.345679012345679,
      "grad_norm": 0.737075924873352,
      "learning_rate": 0.00011499862780544919,
      "loss": 1.2236,
      "step": 2710
    },
    {
      "epoch": 5.3654320987654325,
      "grad_norm": 0.7167155742645264,
      "learning_rate": 0.0001142774058585808,
      "loss": 1.2585,
      "step": 2720
    },
    {
      "epoch": 5.385185185185185,
      "grad_norm": 0.7684142589569092,
      "learning_rate": 0.00011355542499496157,
      "loss": 1.226,
      "step": 2730
    },
    {
      "epoch": 5.404938271604938,
      "grad_norm": 0.6931217312812805,
      "learning_rate": 0.0001128327235915472,
      "loss": 1.2443,
      "step": 2740
    },
    {
      "epoch": 5.424691358024692,
      "grad_norm": 0.7372410893440247,
      "learning_rate": 0.00011210934006359364,
      "loss": 1.2543,
      "step": 2750
    },
    {
      "epoch": 5.444444444444445,
      "grad_norm": 0.705720067024231,
      "learning_rate": 0.0001113853128626152,
      "loss": 1.258,
      "step": 2760
    },
    {
      "epoch": 5.4641975308641975,
      "grad_norm": 0.7195122838020325,
      "learning_rate": 0.00011066068047434088,
      "loss": 1.2294,
      "step": 2770
    },
    {
      "epoch": 5.48395061728395,
      "grad_norm": 0.7444667816162109,
      "learning_rate": 0.00010993548141666825,
      "loss": 1.2256,
      "step": 2780
    },
    {
      "epoch": 5.503703703703704,
      "grad_norm": 0.7265338897705078,
      "learning_rate": 0.00010920975423761636,
      "loss": 1.2416,
      "step": 2790
    },
    {
      "epoch": 5.523456790123457,
      "grad_norm": 0.7177168130874634,
      "learning_rate": 0.00010848353751327656,
      "loss": 1.2533,
      "step": 2800
    },
    {
      "epoch": 5.54320987654321,
      "grad_norm": 0.7259936928749084,
      "learning_rate": 0.00010775686984576196,
      "loss": 1.1823,
      "step": 2810
    },
    {
      "epoch": 5.562962962962963,
      "grad_norm": 0.7342479228973389,
      "learning_rate": 0.00010702978986115578,
      "loss": 1.2447,
      "step": 2820
    },
    {
      "epoch": 5.582716049382716,
      "grad_norm": 0.7165017127990723,
      "learning_rate": 0.00010630233620745777,
      "loss": 1.2596,
      "step": 2830
    },
    {
      "epoch": 5.602469135802469,
      "grad_norm": 0.7511424422264099,
      "learning_rate": 0.00010557454755253028,
      "loss": 1.248,
      "step": 2840
    },
    {
      "epoch": 5.622222222222222,
      "grad_norm": 0.7252684831619263,
      "learning_rate": 0.00010484646258204254,
      "loss": 1.2496,
      "step": 2850
    },
    {
      "epoch": 5.6419753086419755,
      "grad_norm": 0.7123545408248901,
      "learning_rate": 0.00010411811999741445,
      "loss": 1.2396,
      "step": 2860
    },
    {
      "epoch": 5.661728395061728,
      "grad_norm": 0.7307152152061462,
      "learning_rate": 0.00010338955851375962,
      "loss": 1.2141,
      "step": 2870
    },
    {
      "epoch": 5.681481481481481,
      "grad_norm": 0.7177026867866516,
      "learning_rate": 0.00010266081685782701,
      "loss": 1.2724,
      "step": 2880
    },
    {
      "epoch": 5.701234567901235,
      "grad_norm": 0.7345902323722839,
      "learning_rate": 0.00010193193376594279,
      "loss": 1.1938,
      "step": 2890
    },
    {
      "epoch": 5.720987654320988,
      "grad_norm": 0.7366369366645813,
      "learning_rate": 0.00010120294798195114,
      "loss": 1.2305,
      "step": 2900
    },
    {
      "epoch": 5.7407407407407405,
      "grad_norm": 0.7517330050468445,
      "learning_rate": 0.00010047389825515479,
      "loss": 1.2488,
      "step": 2910
    },
    {
      "epoch": 5.760493827160494,
      "grad_norm": 0.7506797909736633,
      "learning_rate": 9.974482333825548e-05,
      "loss": 1.2294,
      "step": 2920
    },
    {
      "epoch": 5.780246913580247,
      "grad_norm": 0.7352750301361084,
      "learning_rate": 9.901576198529382e-05,
      "loss": 1.2439,
      "step": 2930
    },
    {
      "epoch": 5.8,
      "grad_norm": 0.7523927688598633,
      "learning_rate": 9.828675294958949e-05,
      "loss": 1.2332,
      "step": 2940
    },
    {
      "epoch": 5.8197530864197535,
      "grad_norm": 0.7095726728439331,
      "learning_rate": 9.755783498168122e-05,
      "loss": 1.2325,
      "step": 2950
    },
    {
      "epoch": 5.839506172839506,
      "grad_norm": 0.7348549962043762,
      "learning_rate": 9.682904682726698e-05,
      "loss": 1.2538,
      "step": 2960
    },
    {
      "epoch": 5.859259259259259,
      "grad_norm": 0.741853654384613,
      "learning_rate": 9.610042722514459e-05,
      "loss": 1.2436,
      "step": 2970
    },
    {
      "epoch": 5.879012345679012,
      "grad_norm": 0.7277198433876038,
      "learning_rate": 9.537201490515236e-05,
      "loss": 1.2663,
      "step": 2980
    },
    {
      "epoch": 5.898765432098766,
      "grad_norm": 0.7380619645118713,
      "learning_rate": 9.464384858611055e-05,
      "loss": 1.2232,
      "step": 2990
    },
    {
      "epoch": 5.9185185185185185,
      "grad_norm": 0.734936535358429,
      "learning_rate": 9.391596697376327e-05,
      "loss": 1.256,
      "step": 3000
    },
    {
      "epoch": 5.938271604938271,
      "grad_norm": 0.7396767139434814,
      "learning_rate": 9.31884087587209e-05,
      "loss": 1.2623,
      "step": 3010
    },
    {
      "epoch": 5.958024691358025,
      "grad_norm": 0.7331682443618774,
      "learning_rate": 9.246121261440372e-05,
      "loss": 1.2468,
      "step": 3020
    },
    {
      "epoch": 5.977777777777778,
      "grad_norm": 0.7333477139472961,
      "learning_rate": 9.173441719498601e-05,
      "loss": 1.2644,
      "step": 3030
    },
    {
      "epoch": 5.997530864197531,
      "grad_norm": 0.7444273829460144,
      "learning_rate": 9.100806113334157e-05,
      "loss": 1.2337,
      "step": 3040
    },
    {
      "epoch": 6.015802469135802,
      "grad_norm": 0.7352253198623657,
      "learning_rate": 9.028218303899007e-05,
      "loss": 1.2213,
      "step": 3050
    },
    {
      "epoch": 6.035555555555556,
      "grad_norm": 0.7190697193145752,
      "learning_rate": 8.955682149604471e-05,
      "loss": 1.2252,
      "step": 3060
    },
    {
      "epoch": 6.055308641975309,
      "grad_norm": 0.7446117997169495,
      "learning_rate": 8.883201506116154e-05,
      "loss": 1.2189,
      "step": 3070
    },
    {
      "epoch": 6.075061728395061,
      "grad_norm": 0.7331914305686951,
      "learning_rate": 8.810780226148956e-05,
      "loss": 1.2117,
      "step": 3080
    },
    {
      "epoch": 6.094814814814815,
      "grad_norm": 0.7360748648643494,
      "learning_rate": 8.73842215926232e-05,
      "loss": 1.2294,
      "step": 3090
    },
    {
      "epoch": 6.114567901234568,
      "grad_norm": 0.7437692880630493,
      "learning_rate": 8.666131151655592e-05,
      "loss": 1.1996,
      "step": 3100
    },
    {
      "epoch": 6.134320987654321,
      "grad_norm": 0.7590402960777283,
      "learning_rate": 8.593911045963568e-05,
      "loss": 1.189,
      "step": 3110
    },
    {
      "epoch": 6.1540740740740745,
      "grad_norm": 0.7606695294380188,
      "learning_rate": 8.521765681052252e-05,
      "loss": 1.2484,
      "step": 3120
    },
    {
      "epoch": 6.173827160493827,
      "grad_norm": 0.770500659942627,
      "learning_rate": 8.449698891814794e-05,
      "loss": 1.193,
      "step": 3130
    },
    {
      "epoch": 6.19358024691358,
      "grad_norm": 0.7711435556411743,
      "learning_rate": 8.377714508967645e-05,
      "loss": 1.2114,
      "step": 3140
    },
    {
      "epoch": 6.213333333333333,
      "grad_norm": 0.7435481548309326,
      "learning_rate": 8.305816358846942e-05,
      "loss": 1.2044,
      "step": 3150
    },
    {
      "epoch": 6.233086419753087,
      "grad_norm": 0.767671525478363,
      "learning_rate": 8.234008263205108e-05,
      "loss": 1.2158,
      "step": 3160
    },
    {
      "epoch": 6.2528395061728395,
      "grad_norm": 0.7532780766487122,
      "learning_rate": 8.162294039007717e-05,
      "loss": 1.1924,
      "step": 3170
    },
    {
      "epoch": 6.272592592592592,
      "grad_norm": 0.7409538626670837,
      "learning_rate": 8.090677498230596e-05,
      "loss": 1.1995,
      "step": 3180
    },
    {
      "epoch": 6.292345679012346,
      "grad_norm": 0.7674059867858887,
      "learning_rate": 8.019162447657201e-05,
      "loss": 1.2105,
      "step": 3190
    },
    {
      "epoch": 6.312098765432099,
      "grad_norm": 0.7248677611351013,
      "learning_rate": 7.947752688676273e-05,
      "loss": 1.2047,
      "step": 3200
    },
    {
      "epoch": 6.331851851851852,
      "grad_norm": 0.76044100522995,
      "learning_rate": 7.876452017079761e-05,
      "loss": 1.215,
      "step": 3210
    },
    {
      "epoch": 6.351604938271605,
      "grad_norm": 0.7388730645179749,
      "learning_rate": 7.805264222861071e-05,
      "loss": 1.2185,
      "step": 3220
    },
    {
      "epoch": 6.371358024691358,
      "grad_norm": 0.7663106322288513,
      "learning_rate": 7.734193090013595e-05,
      "loss": 1.229,
      "step": 3230
    },
    {
      "epoch": 6.391111111111111,
      "grad_norm": 0.7476632595062256,
      "learning_rate": 7.663242396329585e-05,
      "loss": 1.1945,
      "step": 3240
    },
    {
      "epoch": 6.410864197530864,
      "grad_norm": 0.7591705918312073,
      "learning_rate": 7.592415913199342e-05,
      "loss": 1.2195,
      "step": 3250
    },
    {
      "epoch": 6.4306172839506175,
      "grad_norm": 0.759483277797699,
      "learning_rate": 7.521717405410734e-05,
      "loss": 1.2218,
      "step": 3260
    },
    {
      "epoch": 6.45037037037037,
      "grad_norm": 0.8003749847412109,
      "learning_rate": 7.4511506309491e-05,
      "loss": 1.2345,
      "step": 3270
    },
    {
      "epoch": 6.470123456790123,
      "grad_norm": 0.7730962634086609,
      "learning_rate": 7.380719340797471e-05,
      "loss": 1.1957,
      "step": 3280
    },
    {
      "epoch": 6.489876543209877,
      "grad_norm": 0.7426074743270874,
      "learning_rate": 7.31042727873721e-05,
      "loss": 1.2152,
      "step": 3290
    },
    {
      "epoch": 6.50962962962963,
      "grad_norm": 0.7699944972991943,
      "learning_rate": 7.240278181148987e-05,
      "loss": 1.2136,
      "step": 3300
    },
    {
      "epoch": 6.5293827160493825,
      "grad_norm": 0.7557909488677979,
      "learning_rate": 7.170275776814192e-05,
      "loss": 1.2308,
      "step": 3310
    },
    {
      "epoch": 6.549135802469136,
      "grad_norm": 0.7573074102401733,
      "learning_rate": 7.100423786716723e-05,
      "loss": 1.2343,
      "step": 3320
    },
    {
      "epoch": 6.568888888888889,
      "grad_norm": 0.7570326328277588,
      "learning_rate": 7.030725923845185e-05,
      "loss": 1.2232,
      "step": 3330
    },
    {
      "epoch": 6.588641975308642,
      "grad_norm": 0.7823900580406189,
      "learning_rate": 6.961185892995545e-05,
      "loss": 1.1787,
      "step": 3340
    },
    {
      "epoch": 6.608395061728395,
      "grad_norm": 0.7664496302604675,
      "learning_rate": 6.891807390574199e-05,
      "loss": 1.2253,
      "step": 3350
    },
    {
      "epoch": 6.628148148148148,
      "grad_norm": 0.764736533164978,
      "learning_rate": 6.822594104401472e-05,
      "loss": 1.2368,
      "step": 3360
    },
    {
      "epoch": 6.647901234567901,
      "grad_norm": 0.767412006855011,
      "learning_rate": 6.753549713515621e-05,
      "loss": 1.2022,
      "step": 3370
    },
    {
      "epoch": 6.667654320987654,
      "grad_norm": 0.7500162124633789,
      "learning_rate": 6.684677887977249e-05,
      "loss": 1.202,
      "step": 3380
    },
    {
      "epoch": 6.687407407407408,
      "grad_norm": 0.7587764859199524,
      "learning_rate": 6.615982288674235e-05,
      "loss": 1.2171,
      "step": 3390
    },
    {
      "epoch": 6.7071604938271605,
      "grad_norm": 0.7822524905204773,
      "learning_rate": 6.547466567127135e-05,
      "loss": 1.2338,
      "step": 3400
    },
    {
      "epoch": 6.726913580246913,
      "grad_norm": 0.7826013565063477,
      "learning_rate": 6.479134365295094e-05,
      "loss": 1.2025,
      "step": 3410
    },
    {
      "epoch": 6.746666666666667,
      "grad_norm": 0.7487428188323975,
      "learning_rate": 6.41098931538225e-05,
      "loss": 1.2177,
      "step": 3420
    },
    {
      "epoch": 6.76641975308642,
      "grad_norm": 0.7634575963020325,
      "learning_rate": 6.343035039644655e-05,
      "loss": 1.2019,
      "step": 3430
    },
    {
      "epoch": 6.786172839506173,
      "grad_norm": 0.7473675012588501,
      "learning_rate": 6.275275150197751e-05,
      "loss": 1.2051,
      "step": 3440
    },
    {
      "epoch": 6.805925925925926,
      "grad_norm": 0.7817690372467041,
      "learning_rate": 6.207713248824356e-05,
      "loss": 1.2308,
      "step": 3450
    },
    {
      "epoch": 6.825679012345679,
      "grad_norm": 0.7749472856521606,
      "learning_rate": 6.140352926783218e-05,
      "loss": 1.2042,
      "step": 3460
    },
    {
      "epoch": 6.845432098765432,
      "grad_norm": 0.7735260128974915,
      "learning_rate": 6.073197764618119e-05,
      "loss": 1.2146,
      "step": 3470
    },
    {
      "epoch": 6.865185185185185,
      "grad_norm": 0.7616831660270691,
      "learning_rate": 6.006251331967545e-05,
      "loss": 1.2208,
      "step": 3480
    },
    {
      "epoch": 6.8849382716049385,
      "grad_norm": 0.7818328142166138,
      "learning_rate": 5.93951718737495e-05,
      "loss": 1.2089,
      "step": 3490
    },
    {
      "epoch": 6.904691358024691,
      "grad_norm": 0.7671857476234436,
      "learning_rate": 5.872998878099592e-05,
      "loss": 1.2314,
      "step": 3500
    },
    {
      "epoch": 6.924444444444444,
      "grad_norm": 0.7658620476722717,
      "learning_rate": 5.8066999399279976e-05,
      "loss": 1.2041,
      "step": 3510
    },
    {
      "epoch": 6.944197530864198,
      "grad_norm": 0.7503438591957092,
      "learning_rate": 5.7406238969859904e-05,
      "loss": 1.1877,
      "step": 3520
    },
    {
      "epoch": 6.963950617283951,
      "grad_norm": 0.794966995716095,
      "learning_rate": 5.6747742615513846e-05,
      "loss": 1.1927,
      "step": 3530
    },
    {
      "epoch": 6.9837037037037035,
      "grad_norm": 0.7817770838737488,
      "learning_rate": 5.609154533867279e-05,
      "loss": 1.1875,
      "step": 3540
    },
    {
      "epoch": 7.001975308641975,
      "grad_norm": 0.7468990087509155,
      "learning_rate": 5.5437682019560054e-05,
      "loss": 1.2248,
      "step": 3550
    },
    {
      "epoch": 7.021728395061729,
      "grad_norm": 0.7861433029174805,
      "learning_rate": 5.4786187414337366e-05,
      "loss": 1.1753,
      "step": 3560
    },
    {
      "epoch": 7.0414814814814815,
      "grad_norm": 0.7488176822662354,
      "learning_rate": 5.413709615325708e-05,
      "loss": 1.2166,
      "step": 3570
    },
    {
      "epoch": 7.061234567901234,
      "grad_norm": 0.7745564579963684,
      "learning_rate": 5.349044273882169e-05,
      "loss": 1.1936,
      "step": 3580
    },
    {
      "epoch": 7.080987654320988,
      "grad_norm": 0.7594110369682312,
      "learning_rate": 5.2846261543949674e-05,
      "loss": 1.194,
      "step": 3590
    },
    {
      "epoch": 7.100740740740741,
      "grad_norm": 0.7883632183074951,
      "learning_rate": 5.220458681014852e-05,
      "loss": 1.2155,
      "step": 3600
    },
    {
      "epoch": 7.120493827160494,
      "grad_norm": 0.7997739911079407,
      "learning_rate": 5.156545264569459e-05,
      "loss": 1.1663,
      "step": 3610
    },
    {
      "epoch": 7.140246913580247,
      "grad_norm": 0.7849776148796082,
      "learning_rate": 5.092889302381997e-05,
      "loss": 1.1746,
      "step": 3620
    },
    {
      "epoch": 7.16,
      "grad_norm": 0.7870874404907227,
      "learning_rate": 5.0294941780906756e-05,
      "loss": 1.1601,
      "step": 3630
    },
    {
      "epoch": 7.179753086419753,
      "grad_norm": 0.7604825496673584,
      "learning_rate": 4.9663632614688426e-05,
      "loss": 1.1723,
      "step": 3640
    },
    {
      "epoch": 7.199506172839506,
      "grad_norm": 0.7500194311141968,
      "learning_rate": 4.903499908245862e-05,
      "loss": 1.1523,
      "step": 3650
    },
    {
      "epoch": 7.2192592592592595,
      "grad_norm": 0.8044493198394775,
      "learning_rate": 4.840907459928752e-05,
      "loss": 1.1708,
      "step": 3660
    },
    {
      "epoch": 7.239012345679012,
      "grad_norm": 0.7629950642585754,
      "learning_rate": 4.778589243624544e-05,
      "loss": 1.1983,
      "step": 3670
    },
    {
      "epoch": 7.258765432098765,
      "grad_norm": 0.7582474946975708,
      "learning_rate": 4.71654857186345e-05,
      "loss": 1.191,
      "step": 3680
    },
    {
      "epoch": 7.278518518518519,
      "grad_norm": 0.7674006819725037,
      "learning_rate": 4.654788742422773e-05,
      "loss": 1.2213,
      "step": 3690
    },
    {
      "epoch": 7.298271604938272,
      "grad_norm": 0.7842199206352234,
      "learning_rate": 4.5933130381516234e-05,
      "loss": 1.202,
      "step": 3700
    },
    {
      "epoch": 7.3180246913580245,
      "grad_norm": 0.7841827869415283,
      "learning_rate": 4.532124726796415e-05,
      "loss": 1.2024,
      "step": 3710
    },
    {
      "epoch": 7.337777777777778,
      "grad_norm": 0.7816195487976074,
      "learning_rate": 4.47122706082716e-05,
      "loss": 1.1712,
      "step": 3720
    },
    {
      "epoch": 7.357530864197531,
      "grad_norm": 0.7644449472427368,
      "learning_rate": 4.410623277264592e-05,
      "loss": 1.1746,
      "step": 3730
    },
    {
      "epoch": 7.377283950617284,
      "grad_norm": 0.7622323036193848,
      "learning_rate": 4.350316597508098e-05,
      "loss": 1.1677,
      "step": 3740
    },
    {
      "epoch": 7.397037037037037,
      "grad_norm": 0.7663429379463196,
      "learning_rate": 4.290310227164487e-05,
      "loss": 1.1811,
      "step": 3750
    },
    {
      "epoch": 7.41679012345679,
      "grad_norm": 0.794392466545105,
      "learning_rate": 4.230607355877602e-05,
      "loss": 1.1838,
      "step": 3760
    },
    {
      "epoch": 7.436543209876543,
      "grad_norm": 0.8010687232017517,
      "learning_rate": 4.171211157158758e-05,
      "loss": 1.1781,
      "step": 3770
    },
    {
      "epoch": 7.456296296296296,
      "grad_norm": 0.8054370284080505,
      "learning_rate": 4.112124788218067e-05,
      "loss": 1.2166,
      "step": 3780
    },
    {
      "epoch": 7.47604938271605,
      "grad_norm": 0.7764599323272705,
      "learning_rate": 4.053351389796608e-05,
      "loss": 1.2131,
      "step": 3790
    },
    {
      "epoch": 7.4958024691358025,
      "grad_norm": 0.7735812067985535,
      "learning_rate": 3.9948940859994966e-05,
      "loss": 1.1861,
      "step": 3800
    },
    {
      "epoch": 7.515555555555555,
      "grad_norm": 0.7958599328994751,
      "learning_rate": 3.936755984129793e-05,
      "loss": 1.1978,
      "step": 3810
    },
    {
      "epoch": 7.535308641975309,
      "grad_norm": 0.7810483574867249,
      "learning_rate": 3.878940174523371e-05,
      "loss": 1.2194,
      "step": 3820
    },
    {
      "epoch": 7.555061728395062,
      "grad_norm": 0.7588276267051697,
      "learning_rate": 3.821449730384618e-05,
      "loss": 1.1618,
      "step": 3830
    },
    {
      "epoch": 7.574814814814815,
      "grad_norm": 0.7921299934387207,
      "learning_rate": 3.764287707623091e-05,
      "loss": 1.1985,
      "step": 3840
    },
    {
      "epoch": 7.5945679012345675,
      "grad_norm": 0.8053137063980103,
      "learning_rate": 3.707457144691085e-05,
      "loss": 1.1926,
      "step": 3850
    },
    {
      "epoch": 7.614320987654321,
      "grad_norm": 0.7672552466392517,
      "learning_rate": 3.650961062422122e-05,
      "loss": 1.2033,
      "step": 3860
    },
    {
      "epoch": 7.634074074074074,
      "grad_norm": 0.7951458692550659,
      "learning_rate": 3.5948024638703704e-05,
      "loss": 1.2044,
      "step": 3870
    },
    {
      "epoch": 7.653827160493827,
      "grad_norm": 0.8421968817710876,
      "learning_rate": 3.538984334151021e-05,
      "loss": 1.1895,
      "step": 3880
    },
    {
      "epoch": 7.6735802469135805,
      "grad_norm": 0.8132299184799194,
      "learning_rate": 3.4835096402816124e-05,
      "loss": 1.1999,
      "step": 3890
    },
    {
      "epoch": 7.693333333333333,
      "grad_norm": 0.7980340123176575,
      "learning_rate": 3.428381331024333e-05,
      "loss": 1.2005,
      "step": 3900
    },
    {
      "epoch": 7.713086419753086,
      "grad_norm": 0.7778944373130798,
      "learning_rate": 3.3736023367292513e-05,
      "loss": 1.1859,
      "step": 3910
    },
    {
      "epoch": 7.73283950617284,
      "grad_norm": 0.7838445901870728,
      "learning_rate": 3.319175569178582e-05,
      "loss": 1.1674,
      "step": 3920
    },
    {
      "epoch": 7.752592592592593,
      "grad_norm": 0.7637966275215149,
      "learning_rate": 3.265103921431888e-05,
      "loss": 1.1834,
      "step": 3930
    },
    {
      "epoch": 7.7723456790123455,
      "grad_norm": 0.8065614700317383,
      "learning_rate": 3.2113902676723085e-05,
      "loss": 1.2207,
      "step": 3940
    },
    {
      "epoch": 7.792098765432099,
      "grad_norm": 0.792537271976471,
      "learning_rate": 3.158037463053783e-05,
      "loss": 1.2021,
      "step": 3950
    },
    {
      "epoch": 7.811851851851852,
      "grad_norm": 0.7723316550254822,
      "learning_rate": 3.105048343549287e-05,
      "loss": 1.1655,
      "step": 3960
    },
    {
      "epoch": 7.831604938271605,
      "grad_norm": 0.7848109006881714,
      "learning_rate": 3.052425725800078e-05,
      "loss": 1.1837,
      "step": 3970
    },
    {
      "epoch": 7.851358024691358,
      "grad_norm": 0.7826374769210815,
      "learning_rate": 3.0001724069659786e-05,
      "loss": 1.1895,
      "step": 3980
    },
    {
      "epoch": 7.871111111111111,
      "grad_norm": 0.8083242177963257,
      "learning_rate": 2.9482911645767018e-05,
      "loss": 1.181,
      "step": 3990
    },
    {
      "epoch": 7.890864197530864,
      "grad_norm": 0.8183760046958923,
      "learning_rate": 2.8967847563841975e-05,
      "loss": 1.2013,
      "step": 4000
    },
    {
      "epoch": 7.910617283950617,
      "grad_norm": 0.8031747341156006,
      "learning_rate": 2.845655920216077e-05,
      "loss": 1.2086,
      "step": 4010
    },
    {
      "epoch": 7.930370370370371,
      "grad_norm": 0.7965149879455566,
      "learning_rate": 2.79490737383008e-05,
      "loss": 1.201,
      "step": 4020
    },
    {
      "epoch": 7.9501234567901236,
      "grad_norm": 0.7879059910774231,
      "learning_rate": 2.7445418147696023e-05,
      "loss": 1.1983,
      "step": 4030
    },
    {
      "epoch": 7.969876543209876,
      "grad_norm": 0.7611949443817139,
      "learning_rate": 2.6945619202203187e-05,
      "loss": 1.1933,
      "step": 4040
    },
    {
      "epoch": 7.989629629629629,
      "grad_norm": 0.7487111687660217,
      "learning_rate": 2.64497034686787e-05,
      "loss": 1.1872,
      "step": 4050
    },
    {
      "epoch": 8.007901234567901,
      "grad_norm": 0.7790414690971375,
      "learning_rate": 2.5957697307566586e-05,
      "loss": 1.1637,
      "step": 4060
    },
    {
      "epoch": 8.027654320987654,
      "grad_norm": 0.7743008136749268,
      "learning_rate": 2.546962687149713e-05,
      "loss": 1.1743,
      "step": 4070
    },
    {
      "epoch": 8.047407407407407,
      "grad_norm": 0.7694868445396423,
      "learning_rate": 2.4985518103896833e-05,
      "loss": 1.1659,
      "step": 4080
    },
    {
      "epoch": 8.06716049382716,
      "grad_norm": 0.7793003916740417,
      "learning_rate": 2.4505396737609355e-05,
      "loss": 1.1735,
      "step": 4090
    },
    {
      "epoch": 8.086913580246913,
      "grad_norm": 0.8024742007255554,
      "learning_rate": 2.4029288293527675e-05,
      "loss": 1.2006,
      "step": 4100
    },
    {
      "epoch": 8.106666666666667,
      "grad_norm": 0.8037176728248596,
      "learning_rate": 2.355721807923761e-05,
      "loss": 1.2093,
      "step": 4110
    },
    {
      "epoch": 8.12641975308642,
      "grad_norm": 0.8072035908699036,
      "learning_rate": 2.3089211187672488e-05,
      "loss": 1.1809,
      "step": 4120
    },
    {
      "epoch": 8.146172839506173,
      "grad_norm": 0.8012372255325317,
      "learning_rate": 2.2625292495779338e-05,
      "loss": 1.1759,
      "step": 4130
    },
    {
      "epoch": 8.165925925925926,
      "grad_norm": 0.8181659579277039,
      "learning_rate": 2.2165486663196587e-05,
      "loss": 1.1744,
      "step": 4140
    },
    {
      "epoch": 8.185679012345679,
      "grad_norm": 0.7807929515838623,
      "learning_rate": 2.170981813094327e-05,
      "loss": 1.1459,
      "step": 4150
    },
    {
      "epoch": 8.205432098765431,
      "grad_norm": 0.7993304133415222,
      "learning_rate": 2.12583111201199e-05,
      "loss": 1.1748,
      "step": 4160
    },
    {
      "epoch": 8.225185185185186,
      "grad_norm": 0.7854048609733582,
      "learning_rate": 2.0810989630620926e-05,
      "loss": 1.1613,
      "step": 4170
    },
    {
      "epoch": 8.244938271604939,
      "grad_norm": 0.7950611710548401,
      "learning_rate": 2.0367877439859028e-05,
      "loss": 1.1518,
      "step": 4180
    },
    {
      "epoch": 8.264691358024692,
      "grad_norm": 0.8184006810188293,
      "learning_rate": 1.9928998101501262e-05,
      "loss": 1.1802,
      "step": 4190
    },
    {
      "epoch": 8.284444444444444,
      "grad_norm": 0.7954243421554565,
      "learning_rate": 1.9494374944217042e-05,
      "loss": 1.1796,
      "step": 4200
    },
    {
      "epoch": 8.304197530864197,
      "grad_norm": 0.8178650736808777,
      "learning_rate": 1.906403107043815e-05,
      "loss": 1.1373,
      "step": 4210
    },
    {
      "epoch": 8.32395061728395,
      "grad_norm": 0.7851578593254089,
      "learning_rate": 1.86379893551306e-05,
      "loss": 1.2082,
      "step": 4220
    },
    {
      "epoch": 8.343703703703703,
      "grad_norm": 0.7616697549819946,
      "learning_rate": 1.8216272444578897e-05,
      "loss": 1.194,
      "step": 4230
    },
    {
      "epoch": 8.363456790123458,
      "grad_norm": 0.8082854747772217,
      "learning_rate": 1.779890275518208e-05,
      "loss": 1.1699,
      "step": 4240
    },
    {
      "epoch": 8.38320987654321,
      "grad_norm": 0.8197207450866699,
      "learning_rate": 1.7385902472262296e-05,
      "loss": 1.1796,
      "step": 4250
    },
    {
      "epoch": 8.402962962962963,
      "grad_norm": 0.8223204016685486,
      "learning_rate": 1.6977293548885554e-05,
      "loss": 1.1675,
      "step": 4260
    },
    {
      "epoch": 8.422716049382716,
      "grad_norm": 0.8092452883720398,
      "learning_rate": 1.657309770469473e-05,
      "loss": 1.1873,
      "step": 4270
    },
    {
      "epoch": 8.442469135802469,
      "grad_norm": 0.8053011894226074,
      "learning_rate": 1.617333642475509e-05,
      "loss": 1.1735,
      "step": 4280
    },
    {
      "epoch": 8.462222222222222,
      "grad_norm": 0.781479001045227,
      "learning_rate": 1.577803095841225e-05,
      "loss": 1.1801,
      "step": 4290
    },
    {
      "epoch": 8.481975308641974,
      "grad_norm": 0.8269830346107483,
      "learning_rate": 1.538720231816264e-05,
      "loss": 1.1822,
      "step": 4300
    },
    {
      "epoch": 8.501728395061729,
      "grad_norm": 0.7984444499015808,
      "learning_rate": 1.50008712785367e-05,
      "loss": 1.148,
      "step": 4310
    },
    {
      "epoch": 8.521481481481482,
      "grad_norm": 0.8015949130058289,
      "learning_rate": 1.461905837499441e-05,
      "loss": 1.1885,
      "step": 4320
    },
    {
      "epoch": 8.541234567901235,
      "grad_norm": 0.7935723662376404,
      "learning_rate": 1.4241783902833938e-05,
      "loss": 1.186,
      "step": 4330
    },
    {
      "epoch": 8.560987654320988,
      "grad_norm": 0.8039737343788147,
      "learning_rate": 1.3869067916112644e-05,
      "loss": 1.1794,
      "step": 4340
    },
    {
      "epoch": 8.58074074074074,
      "grad_norm": 0.8029837608337402,
      "learning_rate": 1.350093022658121e-05,
      "loss": 1.1839,
      "step": 4350
    },
    {
      "epoch": 8.600493827160493,
      "grad_norm": 0.7939615845680237,
      "learning_rate": 1.3137390402630623e-05,
      "loss": 1.1758,
      "step": 4360
    },
    {
      "epoch": 8.620246913580246,
      "grad_norm": 0.7812962532043457,
      "learning_rate": 1.2778467768251823e-05,
      "loss": 1.1582,
      "step": 4370
    },
    {
      "epoch": 8.64,
      "grad_norm": 0.7660109996795654,
      "learning_rate": 1.2424181402008672e-05,
      "loss": 1.171,
      "step": 4380
    },
    {
      "epoch": 8.659753086419753,
      "grad_norm": 0.7868942022323608,
      "learning_rate": 1.2074550136023799e-05,
      "loss": 1.1633,
      "step": 4390
    },
    {
      "epoch": 8.679506172839506,
      "grad_norm": 0.8194639086723328,
      "learning_rate": 1.1729592554977552e-05,
      "loss": 1.1955,
      "step": 4400
    },
    {
      "epoch": 8.699259259259259,
      "grad_norm": 0.7842663526535034,
      "learning_rate": 1.138932699512022e-05,
      "loss": 1.1628,
      "step": 4410
    },
    {
      "epoch": 8.719012345679012,
      "grad_norm": 0.8093219995498657,
      "learning_rate": 1.1053771543297198e-05,
      "loss": 1.1447,
      "step": 4420
    },
    {
      "epoch": 8.738765432098765,
      "grad_norm": 0.7955693006515503,
      "learning_rate": 1.0722944035987759e-05,
      "loss": 1.1512,
      "step": 4430
    },
    {
      "epoch": 8.75851851851852,
      "grad_norm": 0.8085020184516907,
      "learning_rate": 1.0396862058356838e-05,
      "loss": 1.1751,
      "step": 4440
    },
    {
      "epoch": 8.778271604938272,
      "grad_norm": 0.7904638648033142,
      "learning_rate": 1.0075542943320294e-05,
      "loss": 1.1699,
      "step": 4450
    },
    {
      "epoch": 8.798024691358025,
      "grad_norm": 0.8035597801208496,
      "learning_rate": 9.759003770623698e-06,
      "loss": 1.1584,
      "step": 4460
    },
    {
      "epoch": 8.817777777777778,
      "grad_norm": 0.80968177318573,
      "learning_rate": 9.447261365934302e-06,
      "loss": 1.1789,
      "step": 4470
    },
    {
      "epoch": 8.83753086419753,
      "grad_norm": 0.7909565567970276,
      "learning_rate": 9.14033229994674e-06,
      "loss": 1.1532,
      "step": 4480
    },
    {
      "epoch": 8.857283950617283,
      "grad_norm": 0.7767083048820496,
      "learning_rate": 8.83823288750224e-06,
      "loss": 1.1835,
      "step": 4490
    },
    {
      "epoch": 8.877037037037038,
      "grad_norm": 0.8150675892829895,
      "learning_rate": 8.540979186721331e-06,
      "loss": 1.1906,
      "step": 4500
    },
    {
      "epoch": 8.89679012345679,
      "grad_norm": 0.8032642006874084,
      "learning_rate": 8.248586998150387e-06,
      "loss": 1.1804,
      "step": 4510
    },
    {
      "epoch": 8.916543209876544,
      "grad_norm": 0.8369510769844055,
      "learning_rate": 7.961071863921598e-06,
      "loss": 1.1966,
      "step": 4520
    },
    {
      "epoch": 8.936296296296296,
      "grad_norm": 0.8156384825706482,
      "learning_rate": 7.678449066926963e-06,
      "loss": 1.1786,
      "step": 4530
    },
    {
      "epoch": 8.95604938271605,
      "grad_norm": 0.8060028553009033,
      "learning_rate": 7.400733630005863e-06,
      "loss": 1.1691,
      "step": 4540
    },
    {
      "epoch": 8.975802469135802,
      "grad_norm": 0.7918094992637634,
      "learning_rate": 7.1279403151464645e-06,
      "loss": 1.1761,
      "step": 4550
    },
    {
      "epoch": 8.995555555555555,
      "grad_norm": 0.7923633456230164,
      "learning_rate": 6.860083622701208e-06,
      "loss": 1.1603,
      "step": 4560
    },
    {
      "epoch": 9.013827160493827,
      "grad_norm": 0.7802884578704834,
      "learning_rate": 6.597177790615872e-06,
      "loss": 1.1755,
      "step": 4570
    },
    {
      "epoch": 9.03358024691358,
      "grad_norm": 0.7694135904312134,
      "learning_rate": 6.3392367936728315e-06,
      "loss": 1.1749,
      "step": 4580
    },
    {
      "epoch": 9.053333333333333,
      "grad_norm": 0.7681608200073242,
      "learning_rate": 6.0862743427482324e-06,
      "loss": 1.1621,
      "step": 4590
    },
    {
      "epoch": 9.073086419753086,
      "grad_norm": 0.8014307618141174,
      "learning_rate": 5.838303884083151e-06,
      "loss": 1.1821,
      "step": 4600
    },
    {
      "epoch": 9.09283950617284,
      "grad_norm": 0.7993208169937134,
      "learning_rate": 5.5953385985689114e-06,
      "loss": 1.1751,
      "step": 4610
    },
    {
      "epoch": 9.112592592592593,
      "grad_norm": 0.8207516670227051,
      "learning_rate": 5.3573914010464074e-06,
      "loss": 1.1819,
      "step": 4620
    },
    {
      "epoch": 9.132345679012346,
      "grad_norm": 0.8299292922019958,
      "learning_rate": 5.124474939619617e-06,
      "loss": 1.156,
      "step": 4630
    },
    {
      "epoch": 9.152098765432099,
      "grad_norm": 0.8167188167572021,
      "learning_rate": 4.89660159498333e-06,
      "loss": 1.1314,
      "step": 4640
    },
    {
      "epoch": 9.171851851851851,
      "grad_norm": 0.7905616760253906,
      "learning_rate": 4.673783479764993e-06,
      "loss": 1.155,
      "step": 4650
    },
    {
      "epoch": 9.191604938271604,
      "grad_norm": 0.8092507123947144,
      "learning_rate": 4.45603243788093e-06,
      "loss": 1.1498,
      "step": 4660
    },
    {
      "epoch": 9.211358024691359,
      "grad_norm": 0.8034681081771851,
      "learning_rate": 4.243360043906719e-06,
      "loss": 1.1532,
      "step": 4670
    },
    {
      "epoch": 9.231111111111112,
      "grad_norm": 0.8061312437057495,
      "learning_rate": 4.035777602461943e-06,
      "loss": 1.1427,
      "step": 4680
    },
    {
      "epoch": 9.250864197530865,
      "grad_norm": 0.8058285713195801,
      "learning_rate": 3.833296147609366e-06,
      "loss": 1.1415,
      "step": 4690
    },
    {
      "epoch": 9.270617283950617,
      "grad_norm": 0.7892848253250122,
      "learning_rate": 3.6359264422683224e-06,
      "loss": 1.1755,
      "step": 4700
    },
    {
      "epoch": 9.29037037037037,
      "grad_norm": 0.8055872917175293,
      "learning_rate": 3.4436789776426948e-06,
      "loss": 1.1458,
      "step": 4710
    },
    {
      "epoch": 9.310123456790123,
      "grad_norm": 0.7801216840744019,
      "learning_rate": 3.2565639726631735e-06,
      "loss": 1.1502,
      "step": 4720
    },
    {
      "epoch": 9.329876543209876,
      "grad_norm": 0.792616605758667,
      "learning_rate": 3.0745913734441355e-06,
      "loss": 1.166,
      "step": 4730
    },
    {
      "epoch": 9.34962962962963,
      "grad_norm": 0.8133671879768372,
      "learning_rate": 2.897770852754944e-06,
      "loss": 1.181,
      "step": 4740
    },
    {
      "epoch": 9.369382716049383,
      "grad_norm": 0.8037221431732178,
      "learning_rate": 2.726111809505738e-06,
      "loss": 1.1558,
      "step": 4750
    },
    {
      "epoch": 9.389135802469136,
      "grad_norm": 0.7874772548675537,
      "learning_rate": 2.559623368247921e-06,
      "loss": 1.1912,
      "step": 4760
    },
    {
      "epoch": 9.408888888888889,
      "grad_norm": 0.8130822777748108,
      "learning_rate": 2.3983143786890704e-06,
      "loss": 1.1503,
      "step": 4770
    },
    {
      "epoch": 9.428641975308642,
      "grad_norm": 0.8210545778274536,
      "learning_rate": 2.24219341522256e-06,
      "loss": 1.1713,
      "step": 4780
    },
    {
      "epoch": 9.448395061728394,
      "grad_norm": 0.7787588238716125,
      "learning_rate": 2.0912687764717775e-06,
      "loss": 1.1485,
      "step": 4790
    },
    {
      "epoch": 9.468148148148147,
      "grad_norm": 0.7965577840805054,
      "learning_rate": 1.9455484848490357e-06,
      "loss": 1.1503,
      "step": 4800
    },
    {
      "epoch": 9.487901234567902,
      "grad_norm": 0.8006945848464966,
      "learning_rate": 1.8050402861291227e-06,
      "loss": 1.1657,
      "step": 4810
    },
    {
      "epoch": 9.507654320987655,
      "grad_norm": 0.8196189999580383,
      "learning_rate": 1.6697516490375654e-06,
      "loss": 1.1646,
      "step": 4820
    },
    {
      "epoch": 9.527407407407408,
      "grad_norm": 0.7993651032447815,
      "learning_rate": 1.5396897648536358e-06,
      "loss": 1.1726,
      "step": 4830
    },
    {
      "epoch": 9.54716049382716,
      "grad_norm": 0.816992461681366,
      "learning_rate": 1.4148615470281123e-06,
      "loss": 1.1867,
      "step": 4840
    },
    {
      "epoch": 9.566913580246913,
      "grad_norm": 0.8137093782424927,
      "learning_rate": 1.2952736308157632e-06,
      "loss": 1.17,
      "step": 4850
    },
    {
      "epoch": 9.586666666666666,
      "grad_norm": 0.8069287538528442,
      "learning_rate": 1.180932372922705e-06,
      "loss": 1.1731,
      "step": 4860
    },
    {
      "epoch": 9.60641975308642,
      "grad_norm": 0.8088211417198181,
      "learning_rate": 1.0718438511684414e-06,
      "loss": 1.1691,
      "step": 4870
    },
    {
      "epoch": 9.626172839506173,
      "grad_norm": 0.8013551235198975,
      "learning_rate": 9.6801386416282e-07,
      "loss": 1.1741,
      "step": 4880
    },
    {
      "epoch": 9.645925925925926,
      "grad_norm": 0.8050767779350281,
      "learning_rate": 8.694479309978576e-07,
      "loss": 1.1858,
      "step": 4890
    },
    {
      "epoch": 9.665679012345679,
      "grad_norm": 0.8329843282699585,
      "learning_rate": 7.761512909542967e-07,
      "loss": 1.1533,
      "step": 4900
    },
    {
      "epoch": 9.685432098765432,
      "grad_norm": 0.8263622522354126,
      "learning_rate": 6.881289032231619e-07,
      "loss": 1.1592,
      "step": 4910
    },
    {
      "epoch": 9.705185185185185,
      "grad_norm": 0.8058146834373474,
      "learning_rate": 6.053854466421372e-07,
      "loss": 1.1826,
      "step": 4920
    },
    {
      "epoch": 9.724938271604938,
      "grad_norm": 0.822230875492096,
      "learning_rate": 5.279253194468425e-07,
      "loss": 1.1967,
      "step": 4930
    },
    {
      "epoch": 9.744691358024692,
      "grad_norm": 0.8152673840522766,
      "learning_rate": 4.557526390370992e-07,
      "loss": 1.1639,
      "step": 4940
    },
    {
      "epoch": 9.764444444444445,
      "grad_norm": 0.8055711388587952,
      "learning_rate": 3.888712417579932e-07,
      "loss": 1.1866,
      "step": 4950
    },
    {
      "epoch": 9.784197530864198,
      "grad_norm": 0.790408730506897,
      "learning_rate": 3.272846826960385e-07,
      "loss": 1.1617,
      "step": 4960
    },
    {
      "epoch": 9.80395061728395,
      "grad_norm": 0.8098512291908264,
      "learning_rate": 2.7099623549012855e-07,
      "loss": 1.1658,
      "step": 4970
    },
    {
      "epoch": 9.823703703703703,
      "grad_norm": 0.8067750334739685,
      "learning_rate": 2.2000889215757492e-07,
      "loss": 1.181,
      "step": 4980
    },
    {
      "epoch": 9.843456790123456,
      "grad_norm": 0.8022135496139526,
      "learning_rate": 1.7432536293504609e-07,
      "loss": 1.1499,
      "step": 4990
    },
    {
      "epoch": 9.863209876543209,
      "grad_norm": 0.8400346040725708,
      "learning_rate": 1.3394807613451575e-07,
      "loss": 1.1454,
      "step": 5000
    },
    {
      "epoch": 9.882962962962964,
      "grad_norm": 0.831190288066864,
      "learning_rate": 9.887917801417734e-08,
      "loss": 1.1986,
      "step": 5010
    },
    {
      "epoch": 9.902716049382716,
      "grad_norm": 0.765605092048645,
      "learning_rate": 6.912053266436846e-08,
      "loss": 1.1707,
      "step": 5020
    },
    {
      "epoch": 9.92246913580247,
      "grad_norm": 0.7971989512443542,
      "learning_rate": 4.467372190846142e-08,
      "loss": 1.1548,
      "step": 5030
    },
    {
      "epoch": 9.942222222222222,
      "grad_norm": 0.8093279004096985,
      "learning_rate": 2.5540045218819253e-08,
      "loss": 1.1819,
      "step": 5040
    },
    {
      "epoch": 9.961975308641975,
      "grad_norm": 0.7937756776809692,
      "learning_rate": 1.172051964767329e-08,
      "loss": 1.1871,
      "step": 5050
    },
    {
      "epoch": 9.981728395061728,
      "grad_norm": 0.792899489402771,
      "learning_rate": 3.215879773132979e-09,
      "loss": 1.1493,
      "step": 5060
    },
    {
      "epoch": 10.0,
      "grad_norm": 1.651845097541809,
      "learning_rate": 2.6577660061644084e-11,
      "loss": 1.194,
      "step": 5070
    }
  ],
  "logging_steps": 10,
  "max_steps": 5070,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.10743919788032e+17,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
